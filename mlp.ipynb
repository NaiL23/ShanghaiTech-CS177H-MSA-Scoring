{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import esm\n",
    "\n",
    "DATASET_PATH = Path() / \"Project\" / \"ShanghaiTech-CS177H-MSA-Scoring\" / \"dataset\" / \"CASP14_fm\"\n",
    "MODEL_PATH = Path() / \"Project\" / \"ShanghaiTech-CS177H-MSA-Scoring\" / \"model\"\n",
    "EMBDEDDINGS_PATH = Path() / \"Project\" / \"ShanghaiTech-CS177H-MSA-Scoring\" / \"embeddings\"\n",
    "TRANSFORMER_PATH = Path() / \"Project\" / \"ShanghaiTech-CS177H-MSA-Scoring\" / \"esm_msa1b_t12_100M_UR50S.pt\"\n",
    "\n",
    "# hyperparameters\n",
    "MAX_DEPTH = 256\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset2 import EScoreDataset\n",
    "train_dataset = EScoreDataset(EMBDEDDINGS_PATH, root = DATASET_PATH, is_train = True)\n",
    "test_dataset = EScoreDataset(EMBDEDDINGS_PATH, root = DATASET_PATH, is_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1024-D1_aug_fm\n",
      "70.208\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.msa_name_list[0])\n",
    "\n",
    "print(train_dataset.msa_score[train_dataset.msa_name_list[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSAPredictor(nn.Module):\n",
    "    def __init__(self, msa_transformer_path = TRANSFORMER_PATH):\n",
    "        super(MSAPredictor, self).__init__()\n",
    "        \"\"\"\n",
    "        if msa_transformer_path:\n",
    "            self.encoder, msa_alphabet = esm.pretrained.load_model_and_alphabet_local(msa_transformer_path)\n",
    "        else :\n",
    "            self.encoder, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "        \n",
    "        self.encoder = self.encoder.eval()\n",
    "        self.batch_converter = msa_alphabet.get_batch_converter()\n",
    "        \"\"\"\n",
    "\n",
    "        # Freeze parameters of MSATransformer\n",
    "        \"\"\"\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \"\"\"\n",
    "\n",
    "        self.em = []\n",
    "\n",
    "        # Regressor module (to be tested)\n",
    "        # self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(3, 3)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(25232, 2048)\n",
    "        # self.fc2 = nn.Linear(2048, 512)\n",
    "        # self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, namelist):\n",
    "        \n",
    "        self.em = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name in namelist:\n",
    "                embedding = test_dataset[name]\n",
    "                xi = torch.mean(embedding[1:, :], dim = 1)\n",
    "                self.em.append(xi)\n",
    "                \n",
    "        x = torch.vstack(self.em)\n",
    "        # BATHCH_SIZE x 768\n",
    "\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset2 import GetRawEmbeddings\n",
    "\n",
    "class MSAPredictorBOS(MSAPredictor):\n",
    "    def __init__(self, msa_transformer_path = TRANSFORMER_PATH):\n",
    "        super(MSAPredictorBOS, self).__init__(msa_transformer_path)\n",
    "        self.em = []\n",
    "        \n",
    "    def forward(self, name_list):\n",
    "        \n",
    "        self.em = []\n",
    "        for name in name_list:\n",
    "            embedding = train_dataset[name]\n",
    "            self.em.append(embedding[0, :])\n",
    "        \n",
    "        x = torch.vstack(self.em)\n",
    "        # BATHCH_SIZE x 768\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bos_256\"\n",
    "\n",
    "if model_name == \"bos_256\" :\n",
    "    model = MSAPredictorBOS().cuda()\n",
    "elif model_name == \"mean_256\" :\n",
    "    model = MSAPredictor().cuda()\n",
    "\n",
    "NUM_GPU = torch.cuda.device_count()\n",
    "USE_PARALLEL = NUM_GPU > 1\n",
    "if USE_PARALLEL :\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset.msa_name_list, batch_size = BATCH_SIZE, num_workers = NUM_GPU * 4, pin_memory = True, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset.msa_name_list, batch_size = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training bos_256 model from the 1st epoch.\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "checkpoint_epoch = 0\n",
    "checkpoint = 0\n",
    "if checkpoint_epoch > 0 :\n",
    "    checkpoint = torch.load(MODEL_PATH / f\"model_{model_name}_epoch{checkpoint_epoch}.pth\")\n",
    "    (model.module if USE_PARALLEL else model).load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"The {model_name} model loaded has been trained for {epoch} epoche(s), with {checkpoint['train_mse']} training loss, {checkpoint['valid_mse']} validation MSE and {checkpoint['test_acc']} test accuracy. \")\n",
    "else :\n",
    "    print(f\"Start training {model_name} model from the 1st epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_metric(model):\n",
    "    model.eval()\n",
    "    mse, correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total = len(test_loader), ncols=80) as bar:\n",
    "            for sample in test_loader :\n",
    "                \n",
    "                y = torch.Tensor([[test_dataset.msa_score[name] / 100.0] for name in sample]).cuda()\n",
    "                pred = model(sample)\n",
    "\n",
    "                mse += torch.sum((y - pred) ** 2).item() \n",
    "                correct += torch.sum(torch.argmax(y) == torch.argmax(pred)).item()\n",
    "                tot += 2\n",
    "\n",
    "                bar.set_postfix({\n",
    "                    \"acc\" : f\"{correct / (tot // 2):.4f}\",\n",
    "                    \"mse\": f\"{mse / tot:.4f}\"\n",
    "                })\n",
    "                bar.update(1)\n",
    "    \n",
    "    return {'accuracy' : correct / (tot // 2), 'mse' : mse / tot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch#1/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [00:05<00:00, 16.36it/s, batch loss=0.08041, loss=0.05630]\n",
      " 15%|██▊                | 14/95 [00:00<00:03, 24.48it/s, acc=0.9286, mse=0.0639]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Project/ShanghaiTech-CS177H-MSA-Scoring/embeddings/T1036s1-D1_our_fm.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         bar\u001b[39m.\u001b[39mset_postfix({\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbatch loss\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlosses \u001b[39m/\u001b[39m tot\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         })\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m         bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m test_metric \u001b[39m=\u001b[39m  calc_test_metric(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m train_mse, test_acc, valid_mse \u001b[39m=\u001b[39m  losses \u001b[39m/\u001b[39m tot, test_metric[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], test_metric[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m torch\u001b[39m.\u001b[39msave({\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m: (model\u001b[39m.\u001b[39mmodule \u001b[39mif\u001b[39;00m USE_PARALLEL \u001b[39melse\u001b[39;00m model)\u001b[39m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m'\u001b[39m: test_acc\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m }, MODEL_PATH \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_epoch\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb Cell 16\u001b[0m in \u001b[0;36mcalc_test_metric\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m test_loader :\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([[test_dataset\u001b[39m.\u001b[39mmsa_score[name] \u001b[39m/\u001b[39m \u001b[39m100.0\u001b[39m] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m sample])\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(sample)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     mse \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum((y \u001b[39m-\u001b[39m pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mitem() \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39margmax(y) \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39margmax(pred))\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb Cell 16\u001b[0m in \u001b[0;36mMSAPredictorBOS.forward\u001b[0;34m(self, name_list)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mem \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m name_list:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     embedding \u001b[39m=\u001b[39m train_dataset[name]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mem\u001b[39m.\u001b[39mappend(embedding[\u001b[39m0\u001b[39m, :])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/tengyue/Project/ShanghaiTech-CS177H-MSA-Scoring/mlp.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mem)\n",
      "File \u001b[0;32m~/Project/ShanghaiTech-CS177H-MSA-Scoring/dataset2.py:56\u001b[0m, in \u001b[0;36mEScoreDataset.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m GetEmbeddings(name, \u001b[39m584\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath)\n",
      "File \u001b[0;32m~/Project/ShanghaiTech-CS177H-MSA-Scoring/dataset2.py:11\u001b[0m, in \u001b[0;36mGetEmbeddings\u001b[0;34m(name, padding, EMBDEDDINGS_PATH)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGetEmbeddings\u001b[39m(name, padding, EMBDEDDINGS_PATH):\n\u001b[0;32m---> 11\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(EMBDEDDINGS_PATH \u001b[39m/\u001b[39;49m  (name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m), map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     12\u001b[0m     rest \u001b[39m=\u001b[39m padding \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m     13\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(data, ((\u001b[39m0\u001b[39m, rest), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Project/ShanghaiTech-CS177H-MSA-Scoring/embeddings/T1036s1-D1_our_fm.pt'"
     ]
    }
   ],
   "source": [
    "for epoch in range(checkpoint_epoch + 1, EPOCHES + 1):\n",
    "    losses, tot = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    with tqdm(total= len(train_loader), ncols=130) as bar:\n",
    "        for batch, sample in enumerate(train_loader) :\n",
    "            i += 1\n",
    "            bar.set_description(f\"[epoch#{epoch}/{EPOCHES}][{batch * BATCH_SIZE}/{len(train_dataset)}]\")\n",
    "\n",
    "            y = torch.Tensor([[train_dataset.msa_score[name] / 100.0] for name in sample]).cuda()\n",
    "            pred = model(sample)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses += loss.item() * len(sample)\n",
    "            tot += len(sample)\n",
    "\n",
    "            bar.set_postfix({\n",
    "                \"batch loss\" : f\"{loss.item():.5f}\",\n",
    "                \"loss\": f\"{losses / tot:.5f}\"\n",
    "            })\n",
    "            bar.update(1)\n",
    "\n",
    "    test_metric =  calc_test_metric(model)\n",
    "    train_mse, test_acc, valid_mse =  losses / tot, test_metric['accuracy'], test_metric['mse']\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': (model.module if USE_PARALLEL else model).state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_mse': train_mse,\n",
    "        'valid_mse': valid_mse,\n",
    "        'test_acc': test_acc\n",
    "    }, MODEL_PATH / f\"model_{model_name}_epoch{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: output the one with more sequences\n",
    "def calc_baseline_test_acc(test_dataset):\n",
    "    correct = 0\n",
    "    pairs = len(test_dataset) // 2\n",
    "    for i in range(pairs) :\n",
    "        msa1, msa2 = test_dataset[2*i], test_dataset[2*i+1]\n",
    "        pred = len(msa1['msa']) > len(msa2['msa'])\n",
    "        gt = msa1['score'] > msa2['score']\n",
    "        if pred == gt : \n",
    "            correct += 1\n",
    "        else :\n",
    "            print(f\"{msa1['name']} of {len(msa1['msa'])} sequences is scored {msa1['score']}.\")\n",
    "            print(f\"{msa2['name']} of {len(msa2['msa'])} sequences is scored {msa2['score']}.\")\n",
    "            print(\"------------\")\n",
    "    return correct / pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(model, only_print_wrong = True):\n",
    "    model.eval()\n",
    "    mse, correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total = len(test_loader), ncols=80, file=sys.stdout) as bar:\n",
    "            for sample in test_loader :\n",
    "                x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "                pred = model(x)\n",
    "\n",
    "                mse += torch.sum((y - pred) ** 2).item() \n",
    "                if torch.argmax(y) == torch.argmax(pred) :\n",
    "                    correct += 1\n",
    "                    if not only_print_wrong:\n",
    "                        for x, y_gt, y_pred in zip(x, y, pred) :\n",
    "                            tqdm.write(f\"{list(x[:, 0]).index(1) if 1 in x[:, 0] else 256:>4d}x{x.size(1):>4d} y_gt:{y_gt.item():.4f}  y_pred:{y_pred.item() * 100:.4f}\")\n",
    "                        tqdm.write(\"======\")\n",
    "                else :\n",
    "                    for x, y_gt, y_pred in zip(x, y, pred) :\n",
    "                        tqdm.write(f\"{list(x[:, 0]).index(1) if 1 in x[:, 0] else 256:>4d}x{x.size(1):>4d} y_gt:{y_gt.item():.4f}  y_pred:{y_pred.item() * 100:.4f}\")\n",
    "                    tqdm.write(\"-------\")\n",
    "                tot += 2\n",
    "\n",
    "                bar.set_postfix({\n",
    "                    \"acc\" : f\"{correct / (tot // 2):.4f}\",\n",
    "                    \"mse\": f\"{mse / tot:.4f}\"\n",
    "                })\n",
    "                bar.update(1)\n",
    "    \n",
    "    return {'accuracy' : correct / (tot // 2), 'mse' : mse / tot}\n",
    "\n",
    "print_predictions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(model_name = \"mean_256\", trained_epoches = 10):\n",
    "    epoches = list(range(1, trained_epoches + 1))\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    for epoch in tqdm(range(1, trained_epoches+1)):\n",
    "        checkpoint = torch.load(MODEL_PATH / f\"model_{model_name}_epoch{epoch}.pth\")\n",
    "        train_losses.append(checkpoint[\"train_mse\"])\n",
    "        test_losses.append(checkpoint[\"valid_mse\"])\n",
    "        test_accs.append(checkpoint[\"test_acc\"])\n",
    "    \n",
    "    trace1 = go.Scatter(\n",
    "        x = epoches,\n",
    "        y = train_losses,\n",
    "        name= \"Train Loss\",\n",
    "        xaxis='x',\n",
    "        yaxis='y1',\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = [epoch + 0.5 for epoch in epoches],\n",
    "        y = test_losses,\n",
    "        name= \"Valid MSE\",\n",
    "        xaxis='x', \n",
    "        yaxis='y1',\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x = epoches,\n",
    "        y = test_accs,\n",
    "        name='Test Accuracy',\n",
    "        xaxis='x', \n",
    "        yaxis='y2',\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "    \n",
    "    data = [trace1, trace2, trace3]\n",
    "    layout = go.Layout(\n",
    "        yaxis2=dict(overlaying = 'y', side = 'right', title = \"Accuracy\", range = [0.85, 1.0]),\n",
    "        yaxis1=dict(title = \"MSE Loss\"),\n",
    "        xaxis = dict(title = \"Epoch\"),\n",
    "        legend=dict(x=0.75, y=0.45, font=dict(size=12, color=\"black\"))\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "plot_training_history(\"bos_256\", 18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "979fbd48e52ed2a14997c40ff6200245796f609ab77113b8ade7c1d89d4140d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
