{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sinfo -O Nodehost,Gres:.30,GresUsed:.45\n",
    "!salloc -N 1 --cpus-per-task=4 -p CS177h --gres=gpu:TeslaM4024GB:1\n",
    "!salloc -N 1 --cpus-per-task=8 -p CS177h --gres=gpu:TeslaM4024GB:2\n",
    "!salloc -N 1 --cpus-per-task=12 -p CS177h --gres=gpu:TeslaM4024GB:3\n",
    "!salloc -N 1 --cpus-per-task=16 -p CS177h --gres=gpu:TeslaM4024GB:4\n",
    "!jupyter-lab --no-brows --ip=0.0.0.0 --port=7774 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fair-esm\n",
    "%conda install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 10 20:41:35 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M40 24GB      On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   27C    P8    27W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla M40 24GB      On   | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P8    27W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla M40 24GB      On   | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   29C    P8    27W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla M40 24GB      On   | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   24C    P8    18W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/public/home/cs177h/lianyh/perl5/project\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import esm\n",
    "\n",
    "DATASET_PATH = Path() / \"perl5\" / \"project\" / \"dataset\" / \"CASP14_fm\"\n",
    "MODEL_PATH = Path() / \"perl5\" / \"project\" / \"model\"\n",
    "EMBDEDDINGS_PATH = Path() / \"perl5\" / \"project\" / \"embdeddings\"\n",
    "TRANSFORMER_PATH = Path() / \"perl5\" / \"project\" / \"esm_msa1b_t12_100M_UR50S.pt\"\n",
    "\n",
    "# hyperparameters\n",
    "MAX_DEPTH = 256\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.a3m` MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T1024-D1:',\n",
       "  'KEFWNLDKNLQLRLGIVFLGAFSYGTVFSSMTIYYNQYLGSAITGILLALSAVATFVAGILAGFFADRNGRKPVMVFGTIIQLLGAALAIASNLPGHVNPWSTFIAFLLISFGYNFVITAGNAMIIDASNAENRKVVFMLDYWAQNLSVILGAALGAWLFRPAFEALLVILLLTVLVSFFLTTFVMTETFKPT'),\n",
       " ('3799131 len:525',\n",
       "  'KGLWALAANTGERFGYTMLAVF---LLYLQANFHYEAGLASTIYSTFLML----VYFLPVIGGIAADRFGFGKMVTTGIFIMFIGYLL-LSIPMGGDYMAIIAMLALILVSLGTGLF--KGNLQVMAPQYADKRDSGFSLFYMAINIGAMFAPTIMDWYYHFAFAVACVSLIVSILIYYFTSSTYN-------'),\n",
       " ('tr|A0A0R1VVR4|A0A0R1VVR4_9LACO Major facilitator superfamily permease OS=Lactobacillus ghanensis DSM 18630 GN=FC89_GL000394 PE=4 SV=1',\n",
       "  'ENIMK--KNLPILLA-SMLVNMGIGLIMPITTLFLHNRLHQTLVTVLMGF-SLAMVLGNLLGGWLFDHWKVKPTHYLGGLLVWLNLTLLIIFPI------WPLY-TILVIGYGFGL----GILNSAIAAHQKKSPNLFTNAYWLANLGMGLATF----NIRWVFSVALFIFIVTLLVVFFHE-----------'),\n",
       " ('8253022 len:434',\n",
       "  'ANFILLV------LG-QGISLFGNTMLFAMSMWVLDETASSTTFATVLAISVIPTILISPFGGVMADRVSKRAMMVISGIVTLLATVF---FALSG----FNILIAIMQVVLAVLDAMETPVVQSAGRESSTDLRRGAAIINQVQQLSQLLPSFLGGVGIRPMMLITAACLMSAAMVECFIRLAKPNQ-----'),\n",
       " ('7093751 len:409',\n",
       "  'KDIRQTDRKIIILLT-VLVDVLGVGIVIPILPFYEENGVSPLVLTLLIAVFSFFSFISAPMLGALSDKIGRRPVLIISIASTAIGWLV---FAWANSI--WMLFLGRIIDGMA------AGNLPVAIAKDEKERTKNLGLISAVFGVGFIIGPALGA-ITLPFFVVGFMALLNTLAAIIFLPETNL-------')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa = read_msa(DATASET_PATH / \"train\" / \"T1024-D1_aug_fm.a3m\")\n",
    "msa[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MSAs\n",
    "\n",
    "read MSA from `.a3m` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_msa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_data_train, _ = read_msa_data(root = DATASET_PATH, is_train = True)\n",
    "msa_data_test, _ = read_msa_data(root = DATASET_PATH, is_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train MSAs: 2660\n",
      "> max MSA depth in train set: 32494\n",
      "> max sequence length in train set: 583\n",
      "# of test MSAs: 190\n",
      "> max MSA depth in test set: 21298\n",
      "> max sequence length in test set: 583\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of train MSAs: {len(msa_data_train)}\")\n",
    "print(f\"> max MSA depth in train set: {max(len(msa['msa']) for _, msa in msa_data_train.items())}\" )\n",
    "print(f\"> max sequence length in train set: {max(len(msa['msa'][0][1]) for _, msa in msa_data_train.items())}\" )\n",
    "\n",
    "print(f\"# of test MSAs: {len(msa_data_test)}\")\n",
    "print(f\"> max MSA depth in test set: {max(len(msa['msa']) for _, msa in msa_data_test.items())}\" )\n",
    "print(f\"> max sequence length in test set: {max(len(msa['msa'][0][1]) for _, msa in msa_data_test.items())}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort MSAs by scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_scored_msas(msa_data, top_k = 10, bot_k = 5) :\n",
    "    msa_scores = sorted([(msa['score'], msa_name) for msa_name, msa in msa_data.items()], reverse = True)\n",
    "    for _, msa_name in msa_scores[:top_k] :\n",
    "        msa = msa_data[msa_name]\n",
    "        print(f\"MSA {msa_name} has { len(msa['msa']) } sequences of length { len(msa['msa'][0][1]) }, scored {msa['score']:.3f}\")\n",
    "    print(\"......\")\n",
    "    for _, msa_name in msa_scores[-bot_k:] :\n",
    "        msa = msa_data[msa_name]\n",
    "        print(f\"MSA {msa_name} has { len(msa['msa']) } sequences of length { len(msa['msa'][0][1]) }, scored {msa['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_rosetta_fm has 10403 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_our_fm has 4277 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_original_fm has 8885 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_meta_fm has 6439 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_deduplicated_fm has 2155 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_cov50_fm has 1956 sequences of length 101, scored 99.505\n",
      "MSA T1101-D1_rand22_fm has 204 sequences of length 83, scored 99.097\n",
      "MSA T1065s2-D1_original_fm has 8868 sequences of length 98, scored 98.980\n",
      "MSA T1101-D1_rand9_fm has 203 sequences of length 83, scored 98.795\n",
      "MSA T1101-D1_rand19_fm has 203 sequences of length 83, scored 98.795\n",
      "......\n",
      "MSA T1026-D1_rand9_fm has 201 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand15_fm has 206 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand7_fm has 200 sequences of length 146, scored 15.755\n",
      "MSA T1043-D1_rand3_fm has 204 sequences of length 148, scored 15.370\n",
      "MSA T1028-D1_rand5_fm has 208 sequences of length 292, scored 13.697\n"
     ]
    }
   ],
   "source": [
    "print_top_scored_msas(msa_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_base_fm has 4808 sequences of length 101, scored 99.752\n",
      "MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "MSA T1065s2-D1_base_fm has 4543 sequences of length 98, scored 99.235\n",
      "MSA T1046s2-D1_rosetta_fm has 52 sequences of length 141, scored 98.935\n",
      "MSA T1070-D4_base_fm has 398 sequences of length 68, scored 98.162\n",
      "MSA T1091-D4_our_fm has 4888 sequences of length 112, scored 97.990\n",
      "MSA T1046s1-D1_rosetta_fm has 44 sequences of length 72, scored 97.915\n",
      "MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "MSA T1052-D3_original_fm has 3201 sequences of length 80, scored 97.812\n",
      "MSA T1089-D1_meta_fm has 9576 sequences of length 377, scored 97.810\n",
      "......\n",
      "MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "MSA T1056-D1_rand15_fm has 203 sequences of length 169, scored 16.273\n",
      "MSA T1038-D1_rand6_fm has 207 sequences of length 114, scored 16.230\n",
      "MSA T1090-D1_rand6_fm has 201 sequences of length 189, scored 15.475\n",
      "MSA T1043-D1_rand1_fm has 200 sequences of length 148, scored 14.525\n"
     ]
    }
   ],
   "source": [
    "print_top_scored_msas(msa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH',\n",
       " '----IEHHHKAAEHHEHAAKHHHAAAEHHQNGDHEKASHHAHAAHGHALHAEHHASEAAKHHANEHG----',\n",
       " '-HKGAEHHHKAAEHHEHAARHHREAAKHYETGNHEKAAHHAHVAHGHHLHARHHAEEATKHHASEHG----',\n",
       " '-HKGAEHHKKAAEHHELAAKHHREAAKHHEAGSHEKGAHHSEIAAGHGLHATYHTEEATKHHAEEHTG---',\n",
       " '--QAAEHHRKAAEHHEHAARHHEEAAEHHEAGKHETAAHHAHLARAHHEVATHHAVEAAKAHLEQHG----']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa = msa_data_test[\"T1084-D1_our_fm\"]['msa']\n",
    "msa_sequences = [sequence for _, sequence in msa]\n",
    "msa_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences (depth) : 2823\n",
      "# of protetins in a sequence: 71\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of sequences (depth) : { len(msa_sequences) }\")\n",
    "print(f\"# of protetins in a sequence: { len(msa_sequences[0]) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequence, sequence_len = 1, 1\n",
    "x = [num_sequence, sequence_len]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sequences in an MSA share the same number of protetins.\n"
     ]
    }
   ],
   "source": [
    "def same_length(msa_data):\n",
    "    return all(all([len(sequence) == len(msa['msa'][0]) for sequence in msa['msa']]) for msa in msa_data.values())\n",
    "\n",
    "if same_length(msa_data_test) and same_length(msa_data_train):\n",
    "    print(\"All sequences in an MSA share the same number of protetins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling MSA\n",
    "\n",
    "Subsampling MSA = swallowing MSA depth = decreasing # of sequences in an MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HH Filter\n",
    "\n",
    "We can see the number of sequences (also called MSA depth) of MSAs varies and some are very deep. Furthermore, the MSA Transformer only supports maximum MSA depth $\\leq 1024$. \n",
    "\n",
    "We try to cut the number of sequences of deeper MSAs to 256 each. A simple approach is to intercept the first 256 sequences. However, let's try the method called **HH Filter** with the `-diff 256` parameter, which can extract a representative set (256 or more, usually close to 256) of the sequences from an alignment. If more than 256 sequences are returned we apply the Diversity Maximizing strategy on the HHFilter output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge -c bioconda hhsuite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run HHFilter in command line like `hhfilter -i <INTPUT> -o <OUTPUT> [options] `. See https://manpages.ubuntu.com/manpages/bionic/man1/hhfilter.1.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 15:47:19.878 INFO: Input file = perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m\n",
      "\n",
      "- 15:47:19.878 INFO: Output file = perl5/project/dataset/CASP14_fm/test_filtered/T1084-D1_our_fm.a3m\n",
      "\n",
      "- 15:47:19.990 DEBUG: Read perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m with 2823 sequences\n",
      "\n",
      "- 15:47:19.990 DEBUG: Single sequence in file perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m contains only 71 match_states! Switching to option -M first\n",
      " seq= AHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH\n",
      "\n",
      "- 15:47:19.993 DEBUG: Alignment in perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m contains 71 match states\n",
      "\n",
      "- 15:47:19.993 DEBUG: Using the Gonnet matrix\n",
      "\n",
      "- 15:47:19.993 DEBUG: sequence identity = 17.3066 %; entropy per column = 3.93717 bits (out of 4.18529); mutual information = 0.248121 bits\n",
      "\n",
      "- 15:47:20.001 DEBUG: 281 out of 2823 sequences passed filter (\n",
      "- 15:47:20.001 DEBUG: up to 25% position-dependent max pairwise sequence identity)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hhfilter -i perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m -o perl5/project/dataset/CASP14_fm/test_filtered/T1084-D1_our_fm.a3m -diff 256 -v 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do this via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- 15:53:50.158 INFO: Input file = perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m\n",
      "\n",
      "- 15:53:50.158 INFO: Output file = perl5/project/dataset/CASP14_fm/test_filtered/T1084-D1_our_fm.a3m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subsampling import hh_filter\n",
    "hh_filter(\"T1084-D1_our_fm.a3m\", DATASET_PATH / \"test\", DATASET_PATH / \"test_filtered\", diff=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see 281 (close to 256) sequences are filtered out of all 2823 sequences. Now let's apply for all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsampling import filter_msa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2660/2660 [01:30<00:00, 29.43it/s]\n"
     ]
    }
   ],
   "source": [
    "filter_msa_data(msa_data_train, root = DATASET_PATH, is_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:15<00:00, 12.24it/s]\n"
     ]
    }
   ],
   "source": [
    "filter_msa_data(msa_data_test, root = DATASET_PATH, is_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see MSA depths are much better but some are still $>256$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_rosetta_fm has 267 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_our_fm has 285 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_original_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_meta_fm has 485 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_deduplicated_fm has 437 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_cov50_fm has 388 sequences of length 101, scored 99.505\n",
      "MSA T1101-D1_rand22_fm has 204 sequences of length 83, scored 99.097\n",
      "MSA T1065s2-D1_original_fm has 314 sequences of length 98, scored 98.980\n",
      "MSA T1101-D1_rand9_fm has 203 sequences of length 83, scored 98.795\n",
      "MSA T1101-D1_rand19_fm has 203 sequences of length 83, scored 98.795\n",
      "......\n",
      "MSA T1026-D1_rand9_fm has 201 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand15_fm has 206 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand7_fm has 200 sequences of length 146, scored 15.755\n",
      "MSA T1043-D1_rand3_fm has 204 sequences of length 148, scored 15.370\n",
      "MSA T1028-D1_rand5_fm has 208 sequences of length 292, scored 13.697\n"
     ]
    }
   ],
   "source": [
    "msa_data_train, _ = read_msa_data(is_train = True, is_filtered = True)\n",
    "print_top_scored_msas(msa_data_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_base_fm has 420 sequences of length 101, scored 99.752\n",
      "MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "MSA T1065s2-D1_base_fm has 331 sequences of length 98, scored 99.235\n",
      "MSA T1046s2-D1_rosetta_fm has 52 sequences of length 141, scored 98.935\n",
      "MSA T1070-D4_base_fm has 227 sequences of length 68, scored 98.162\n",
      "MSA T1091-D4_our_fm has 556 sequences of length 112, scored 97.990\n",
      "MSA T1046s1-D1_rosetta_fm has 44 sequences of length 72, scored 97.915\n",
      "MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "MSA T1052-D3_original_fm has 273 sequences of length 80, scored 97.812\n",
      "MSA T1089-D1_meta_fm has 455 sequences of length 377, scored 97.810\n",
      "......\n",
      "MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "MSA T1056-D1_rand15_fm has 203 sequences of length 169, scored 16.273\n",
      "MSA T1038-D1_rand6_fm has 207 sequences of length 114, scored 16.230\n",
      "MSA T1090-D1_rand6_fm has 201 sequences of length 189, scored 15.475\n",
      "MSA T1043-D1_rand1_fm has 200 sequences of length 148, scored 14.525\n"
     ]
    }
   ],
   "source": [
    "msa_data_test, _ = read_msa_data(is_train = False, is_filtered = True)\n",
    "print_top_scored_msas(msa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity Maxmizing\n",
    "\n",
    "Diversity Maximizing is a greedy strategy which starts from the reference and adds the sequence with highest average hamming distance to current set of sequences. Hence we can cut each MSA to at most 256 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsampling import greedy_subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before subsampling: 281\n"
     ]
    }
   ],
   "source": [
    "msa = read_msa(DATASET_PATH / \"test_filtered\" / \"T1084-D1_our_fm.a3m\")\n",
    "print(f\"before subsampling: { len(msa) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after subsampling: 256\n"
     ]
    }
   ],
   "source": [
    "msa_subsampled = greedy_subsampling(msa)\n",
    "print(f\"after subsampling: { len(msa_subsampled) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply for all MSAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsampling import subsampling_msa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_rosetta_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_our_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_original_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_meta_fm has 256 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_deduplicated_fm has 256 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_cov50_fm has 256 sequences of length 101, scored 99.505\n",
      "MSA T1101-D1_rand22_fm has 204 sequences of length 83, scored 99.097\n",
      "MSA T1065s2-D1_original_fm has 256 sequences of length 98, scored 98.980\n",
      "MSA T1101-D1_rand9_fm has 203 sequences of length 83, scored 98.795\n",
      "MSA T1101-D1_rand19_fm has 203 sequences of length 83, scored 98.795\n",
      "......\n",
      "MSA T1026-D1_rand9_fm has 201 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand15_fm has 206 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand7_fm has 200 sequences of length 146, scored 15.755\n",
      "MSA T1043-D1_rand3_fm has 204 sequences of length 148, scored 15.370\n",
      "MSA T1028-D1_rand5_fm has 208 sequences of length 292, scored 13.697\n"
     ]
    }
   ],
   "source": [
    "msa_data_train = subsampling_msa_data(msa_data_train)\n",
    "print_top_scored_msas(msa_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_base_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "MSA T1065s2-D1_base_fm has 256 sequences of length 98, scored 99.235\n",
      "MSA T1046s2-D1_rosetta_fm has 52 sequences of length 141, scored 98.935\n",
      "MSA T1070-D4_base_fm has 227 sequences of length 68, scored 98.162\n",
      "MSA T1091-D4_our_fm has 256 sequences of length 112, scored 97.990\n",
      "MSA T1046s1-D1_rosetta_fm has 44 sequences of length 72, scored 97.915\n",
      "MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "MSA T1052-D3_original_fm has 256 sequences of length 80, scored 97.812\n",
      "MSA T1089-D1_meta_fm has 256 sequences of length 377, scored 97.810\n",
      "......\n",
      "MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "MSA T1056-D1_rand15_fm has 203 sequences of length 169, scored 16.273\n",
      "MSA T1038-D1_rand6_fm has 207 sequences of length 114, scored 16.230\n",
      "MSA T1090-D1_rand6_fm has 201 sequences of length 189, scored 15.475\n",
      "MSA T1043-D1_rand1_fm has 200 sequences of length 148, scored 14.525\n"
     ]
    }
   ],
   "source": [
    "msa_data_test = subsampling_msa_data(msa_data_test)\n",
    "print_top_scored_msas(msa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 2660/2660 [00:39<00:00, 66.53it/s]\n"
     ]
    }
   ],
   "source": [
    "msa_data_train, _ = read_msa_data(is_train = True, is_filtered = True)\n",
    "msa_data_train = subsampling_msa_data(msa_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:06<00:00, 27.93it/s]\n"
     ]
    }
   ],
   "source": [
    "msa_data_test, _ = read_msa_data(is_train = False, is_filtered = True)\n",
    "msa_data_test = subsampling_msa_data(msa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load MSA Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_TRANSFORMER = False\n",
    "if DOWNLOAD_TRANSFORMER :\n",
    "    msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "else :\n",
    "    msa_transformer, msa_alphabet = esm.pretrained.load_model_and_alphabet_local(TRANSFORMER_PATH)\n",
    "\n",
    "# USE_PARALLEL = torch.cuda.device_count() > 1\n",
    "# # USE_PARALLEL = False\n",
    "# if USE_PARALLEL :\n",
    "#     msa_transformer = torch.nn.DataParallel(msa_transformer)\n",
    "    \n",
    "msa_transformer = msa_transformer.eval().cuda()\n",
    "msa_batch_converter = msa_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet size: 33\n",
      "padding token: 1\n",
      "begin of sequence token : 0\n",
      "end of sequence token (not used here) : 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"alphabet size: {len(msa_alphabet)}\")\n",
    "print(f\"padding token: {msa_alphabet.padding_idx}\")\n",
    "print(f\"begin of sequence token : {msa_alphabet.cls_idx}\")\n",
    "print(f\"end of sequence token (not used here) : {msa_alphabet.eos_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSATransformer(\n",
       "  (embed_tokens): Embedding(33, 768, padding_idx=1)\n",
       "  (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (6): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (7): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (9): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (10): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (11): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=144, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "  (emb_layer_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (emb_layer_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 267, 194]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from dataset import read_msa\n",
    "msa_data = [\n",
    "    read_msa(DATASET_PATH / \"train_filtered\" / \"T1024-D1_aug_fm.a3m\"),\n",
    "    read_msa(DATASET_PATH / \"train_filtered\" / \"T1024-D1_base_fm.a3m\"),\n",
    "    #read_msa(DATASET_PATH / \"train_filtered\" / \"T1101-D2_rand22_fm.a3m\")\n",
    "]\n",
    "\n",
    "msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(msa_data)\n",
    "print(msa_batch_tokens.shape, msa_batch_tokens.dtype)   # Should be a 3D tensor with dtype torch.int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0, 15,  9,  ..., 15, 14, 11],\n",
      "         [ 0, 15,  6,  ..., 30, 30, 30],\n",
      "         [ 0,  9, 17,  ..., 30, 30, 30],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 15,  9,  ..., 15, 14, 11],\n",
      "         [ 0, 30, 30,  ..., 30, 30, 30],\n",
      "         [ 0, 30, 30,  ..., 30, 11, 30],\n",
      "         ...,\n",
      "         [ 0, 30, 30,  ..., 14, 14, 30],\n",
      "         [ 0, 30, 30,  ..., 10,  5, 30],\n",
      "         [ 0, 30, 30,  ..., 30, 30, 30]]])\n"
     ]
    }
   ],
   "source": [
    "print(msa_batch_tokens)\n",
    "# 0: beginning of sequence\n",
    "# 1: empty\n",
    "# 30: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max MSA depth (# of sequences): 256\n",
      "max length of each sequence: 193\n"
     ]
    }
   ],
   "source": [
    "print(f\"max MSA depth (# of sequences): { msa_batch_tokens.shape[1] }\")\n",
    "print(f\"max length of each sequence: { msa_batch_tokens.shape[2]-1 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of model layers: 12\n",
      "model embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of model layers: {len((msa_transformer.module if USE_PARALLEL else msa_transformer).layers)}\")\n",
    "print(f\"model embedding dimension: {(msa_transformer.module if USE_PARALLEL else msa_transformer).layers[0].embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer.eval()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    results = msa_transformer(msa_batch_tokens.cuda(non_blocking = True), repr_layers=[12])\n",
    "    #results = msa_transformer(msa_batch_tokens.cuda(), repr_layers=[12], need_head_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 214, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = results['representations'][12][:, 0, 1:, :]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15,  9, 18, 22, 17,  4, 13, 15, 17,  4, 16,  4, 10,  4,  6, 12,  7, 18,\n",
      "         4,  6,  5, 18,  8, 19,  6, 11,  7, 18,  8,  8, 20, 11, 12, 19, 19, 17,\n",
      "        16, 19,  4,  6,  8,  5, 12, 11,  6, 12,  4,  4,  5,  4,  8,  5,  7,  5,\n",
      "        11, 18,  7,  5,  6, 12,  4,  5,  6, 18, 18,  5, 13, 10, 17,  6, 10, 15,\n",
      "        14,  7, 20,  7, 18,  6, 11, 12, 12, 16,  4,  4,  6,  5,  5,  4,  5, 12,\n",
      "         5,  8, 17,  4, 14,  6, 21,  7, 17, 14, 22,  8, 11, 18, 12,  5, 18,  4,\n",
      "         4, 12,  8, 18,  6, 19, 17, 18,  7, 12, 11,  5,  6, 17,  5, 20, 12, 12,\n",
      "        13,  5,  8, 17,  5,  9, 17, 10, 15,  7,  7, 18, 20,  4, 13, 19, 22,  5,\n",
      "        16, 17,  4,  8,  7, 12,  4,  6,  5,  5,  4,  6,  5, 22,  4, 18, 10, 14,\n",
      "         5, 18,  9,  5,  4,  4,  7, 12,  4,  4,  4, 11,  7,  4,  7,  8, 18, 18,\n",
      "         4, 11, 11, 18,  7, 20, 11,  9, 11, 18, 15, 14, 11])\n",
      "tensor([15,  9, 18, 22, 17,  4, 13, 15, 17,  4, 16,  4, 10,  4,  6, 12,  7, 18,\n",
      "         4,  6,  5, 18,  8, 19,  6, 11,  7, 18,  8,  8, 20, 11, 12, 19, 19, 17,\n",
      "        16,  6,  4,  6,  8,  5, 12, 11,  6, 12,  4,  4,  5,  4,  8,  5,  7,  5,\n",
      "        11, 18,  7,  5,  6, 12,  4,  5,  6, 18, 18,  5, 13, 10, 18,  6, 10, 15,\n",
      "        14,  7, 20,  7, 18,  6, 11, 12, 12, 16,  4,  4,  6,  5,  5,  4,  5, 12,\n",
      "         5, 18, 14,  4, 14,  6, 21,  7, 17, 14, 22,  8, 11, 18, 12,  5, 18,  4,\n",
      "         4, 12,  8, 18,  6, 19,  6, 18,  7, 12, 11,  5,  6, 17,  5, 16, 12, 12,\n",
      "        13,  5,  8, 17,  5,  9, 17, 10, 15,  7,  7, 18,  8,  4, 13, 19, 22,  5,\n",
      "        16, 17,  4,  8,  7, 12,  4,  6,  5,  5,  4,  6,  5, 22,  4, 18, 10, 14,\n",
      "         5, 18, 12,  5,  6,  4,  7, 12,  4,  4,  4, 11,  7,  4,  7,  8, 18, 18,\n",
      "         4, 11, 11, 15, 15, 15, 11,  9, 11, 18, 15, 14, 11], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(msa_batch_tokens[0][0][1:])\n",
    "\n",
    "print(torch.argmax(results['logits'][0][0][1:], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MSAScoreDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2660/2660 [00:36<00:00, 72.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2660 MSAs:\n",
      "> MSA T1024-D1_aug_fm has 97 sequences of length 193, scored 70.208\n",
      "> MSA T1024-D1_base_fm has 256 sequences of length 193, scored 96.112\n",
      "> MSA T1024-D1_cov50_fm has 256 sequences of length 193, scored 94.430\n",
      "> MSA T1024-D1_deduplicated_fm has 256 sequences of length 193, scored 88.213\n",
      "> MSA T1024-D1_original_fm has 256 sequences of length 193, scored 96.112\n",
      "> MSA T1024-D1_our_fm has 256 sequences of length 193, scored 95.983\n",
      "> MSA T1024-D1_rand11_fm has 207 sequences of length 193, scored 60.102\n",
      "> MSA T1024-D1_rand12_fm has 205 sequences of length 193, scored 56.993\n",
      "> MSA T1024-D1_rand13_fm has 207 sequences of length 193, scored 73.705\n",
      "> MSA T1024-D1_rand14_fm has 201 sequences of length 193, scored 62.565\n",
      "......\n",
      "> MSA T1101-D2_rand22_fm has 202 sequences of length 214, scored 26.517\n",
      "> MSA T1101-D2_rand2_fm has 204 sequences of length 214, scored 31.190\n",
      "> MSA T1101-D2_rand3_fm has 205 sequences of length 214, scored 43.225\n",
      "> MSA T1101-D2_rand4_fm has 200 sequences of length 214, scored 26.752\n",
      "> MSA T1101-D2_rand5_fm has 201 sequences of length 214, scored 78.970\n",
      "> MSA T1101-D2_rand6_fm has 201 sequences of length 214, scored 26.517\n",
      "> MSA T1101-D2_rand7_fm has 208 sequences of length 214, scored 76.752\n",
      "> MSA T1101-D2_rand8_fm has 202 sequences of length 214, scored 56.773\n",
      "> MSA T1101-D2_rand9_fm has 200 sequences of length 214, scored 33.295\n",
      "> MSA T1101-D2_rosetta_fm has 256 sequences of length 214, scored 93.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MSAScoreDataset(root = DATASET_PATH, is_train = True)\n",
    "train_dataset.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:06<00:00, 29.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 190 MSAs:\n",
      "> MSA T1024-D1_meta_fm has 256 sequences of length 193, scored 96.243\n",
      "> MSA T1024-D1_rand10_fm has 206 sequences of length 193, scored 56.865\n",
      "> MSA T1024-D2_original_fm has 256 sequences of length 198, scored 95.580\n",
      "> MSA T1024-D2_rand13_fm has 203 sequences of length 198, scored 92.803\n",
      "> MSA T1025-D1_our_fm has 256 sequences of length 257, scored 92.315\n",
      "> MSA T1025-D1_rand1_fm has 202 sequences of length 257, scored 57.297\n",
      "> MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "> MSA T1026-D1_rosetta_fm has 19 sequences of length 146, scored 93.325\n",
      "> MSA T1027-D1_cov50_fm has 256 sequences of length 99, scored 65.153\n",
      "> MSA T1027-D1_rand18_fm has 203 sequences of length 99, scored 26.513\n",
      "......\n",
      "> MSA T1099-D1_cov50_fm has 256 sequences of length 178, scored 27.527\n",
      "> MSA T1099-D1_rosetta_fm has 8 sequences of length 178, scored 80.058\n",
      "> MSA T1100-D1_rand13_fm has 200 sequences of length 169, scored 60.800\n",
      "> MSA T1100-D1_rosetta_fm has 256 sequences of length 169, scored 67.750\n",
      "> MSA T1100-D2_base_fm has 256 sequences of length 157, scored 97.453\n",
      "> MSA T1100-D2_rand14_fm has 205 sequences of length 157, scored 22.453\n",
      "> MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "> MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "> MSA T1101-D2_base_fm has 256 sequences of length 214, scored 93.458\n",
      "> MSA T1101-D2_rand12_fm has 205 sequences of length 214, scored 25.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MSAScoreDataset(root = DATASET_PATH, is_train = False)\n",
    "test_dataset.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSAPredictor(nn.Module):\n",
    "    def __init__(self, msa_transformer_path = TRANSFORMER_PATH):\n",
    "        super(MSAPredictor, self).__init__()\n",
    "        \n",
    "        if msa_transformer_path:\n",
    "            self.encoder, msa_alphabet = esm.pretrained.load_model_and_alphabet_local(msa_transformer_path)\n",
    "        else :\n",
    "            self.encoder, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "        \n",
    "        self.encoder = self.encoder.eval()\n",
    "        self.batch_converter = msa_alphabet.get_batch_converter()\n",
    "\n",
    "        # Freeze parameters of MSATransformer\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Regressor module (to be tested)\n",
    "        # self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(3, 3)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(25232, 2048)\n",
    "        # self.fc2 = nn.Linear(2048, 512)\n",
    "        # self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_length = [list(x[i, 0]).index(1) if 1 in x[i, 0] else x.size(2) for i in range(x.size(0))]\n",
    "        \n",
    "        self.encoder.eval()\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(x.size(0)) :\n",
    "                # Remove sequence paddings\n",
    "                xi = x[i:i+1, :, :seq_length[i]]\n",
    "                # 1 x NUM_SEQ x 1+SEQ_LEN\n",
    "\n",
    "                # xx = xx[:, :list(x[i, :, 0]).index(1), :] if 1 in x[i, :, 0] else xx\n",
    "\n",
    "                xi = self.encoder(xi, repr_layers=[12])[\"representations\"][12][:, 0, 1:, :]\n",
    "                # 1 x SEQ_LEN x 768\n",
    "\n",
    "                xi = torch.mean(xi, dim = 1)\n",
    "                # 1 x 768\n",
    "                embeddings.append(xi)\n",
    "                \n",
    "        x = torch.vstack(embeddings)\n",
    "        # BATHCH_SIZE x 768\n",
    "\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSAPredictorBOS(MSAPredictor):\n",
    "    def __init__(self, msa_transformer_path = TRANSFORMER_PATH):\n",
    "        super(MSAPredictorBOS, self).__init__(msa_transformer_path)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_length = [list(x[i, 0]).index(1) if 1 in x[i, 0] else x.size(2) for i in range(x.size(0))]\n",
    "        \n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(x.size(0)) :\n",
    "                # Remove sequence paddings\n",
    "                xi = x[i:i+1, :, :seq_length[i]]\n",
    "                # 1 x NUM_SEQ x 1+SEQ_LEN\n",
    "\n",
    "                xi = self.encoder(xi, repr_layers=[12])[\"representations\"][12][:, 0, 0, :]\n",
    "                # 1 x 768\n",
    "                \n",
    "                embeddings.append(xi)\n",
    "                \n",
    "        x = torch.vstack(embeddings)\n",
    "        # BATHCH_SIZE x 768\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MSAPredictorBOS(\n",
       "    (encoder): MSATransformer(\n",
       "      (embed_tokens): Embedding(33, 768, padding_idx=1)\n",
       "      (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (contact_head): ContactPredictionHead(\n",
       "        (regression): Linear(in_features=144, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "      (emb_layer_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (emb_layer_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bos_256\"\n",
    "\n",
    "if model_name == \"bos_256\" :\n",
    "    model = MSAPredictorBOS().cuda()\n",
    "elif model_name == \"mean_256\" :\n",
    "    model = MSAPredictor().cuda()\n",
    "\n",
    "batch_converter = model.batch_converter\n",
    "\n",
    "NUM_GPU = torch.cuda.device_count()\n",
    "USE_PARALLEL = NUM_GPU > 1\n",
    "if USE_PARALLEL :\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_collate_fn = lambda batch: {'msa': batch_converter([msa['msa'] for msa in batch])[2], 'score': torch.Tensor([[msa['score'] / 100.0] for msa in batch])}\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, collate_fn = batch_collate_fn, num_workers = NUM_GPU * 4, pin_memory = True, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 2, collate_fn = batch_collate_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msa': tensor([[[ 0,  7,  8,  ...,  1,  1,  1],\n",
      "         [ 0, 20,  8,  ...,  1,  1,  1],\n",
      "         [ 0,  7,  8,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 20, 18,  ...,  1,  1,  1],\n",
      "         [ 0, 20, 12,  ...,  1,  1,  1],\n",
      "         [ 0, 20, 20,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 10, 16,  ...,  1,  1,  1],\n",
      "         [ 0, 10, 16,  ...,  1,  1,  1],\n",
      "         [ 0, 10, 16,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0, 20, 12,  ...,  1,  1,  1],\n",
      "         [ 0,  4, 16,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 17, 15,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 13, 18,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 18,  7,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1]]]), 'score': tensor([[0.2632],\n",
      "        [0.8375],\n",
      "        [0.2826],\n",
      "        [0.4748],\n",
      "        [0.3368],\n",
      "        [0.2607],\n",
      "        [0.8644],\n",
      "        [0.9269],\n",
      "        [0.7735],\n",
      "        [0.2418],\n",
      "        [0.1687],\n",
      "        [0.9816],\n",
      "        [0.2040],\n",
      "        [0.9819],\n",
      "        [0.9688],\n",
      "        [0.8852],\n",
      "        [0.9618],\n",
      "        [0.5280],\n",
      "        [0.5334],\n",
      "        [0.8255],\n",
      "        [0.6201],\n",
      "        [0.2677],\n",
      "        [0.9443],\n",
      "        [0.8551],\n",
      "        [0.9332],\n",
      "        [0.3786],\n",
      "        [0.4255],\n",
      "        [0.5912],\n",
      "        [0.4162],\n",
      "        [0.5352],\n",
      "        [0.8554],\n",
      "        [0.6035]])}\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "print(next(dataiter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bos_256 model loaded has been trained for 18 epoche(s), with 0.02081390372232387 training loss, 0.02062851033253154 validation MSE and 0.9578947368421052 test accuracy. \n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "checkpoint_epoch = 18\n",
    "if checkpoint_epoch > 0 :\n",
    "    checkpoint = torch.load(MODEL_PATH / f\"model_{model_name}_epoch{checkpoint_epoch}.pth\")\n",
    "    (model.module if USE_PARALLEL else model).load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"The {model_name} model loaded has been trained for {epoch} epoche(s), with {checkpoint['train_mse']} training loss, {checkpoint['valid_mse']} validation MSE and {checkpoint['test_acc']} test accuracy. \")\n",
    "else :\n",
    "    print(f\"Start training {model_name} model from the 1st epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_metric(model):\n",
    "    model.eval()\n",
    "    mse, correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total = len(test_loader), ncols=80) as bar:\n",
    "            for sample in test_loader :\n",
    "                x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "                pred = model(x)\n",
    "\n",
    "                mse += torch.sum((y - pred) ** 2).item() \n",
    "                correct += torch.sum(torch.argmax(y) == torch.argmax(pred)).item()\n",
    "                tot += 2\n",
    "\n",
    "                bar.set_postfix({\n",
    "                    \"acc\" : f\"{correct / (tot // 2):.4f}\",\n",
    "                    \"mse\": f\"{mse / tot:.4f}\"\n",
    "                })\n",
    "                bar.update(1)\n",
    "    \n",
    "    return {'accuracy' : correct / (tot // 2), 'mse' : mse / tot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch#1/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:37<00:00, 36.16s/it, batch loss=0.07708, loss=0.05778]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.8947, mse=0.0675]\n",
      "[epoch#2/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:03<00:00, 35.75s/it, batch loss=0.03499, loss=0.04882]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9368, mse=0.0520]\n",
      "[epoch#3/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:11<00:00, 35.85s/it, batch loss=0.06709, loss=0.04081]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9579, mse=0.0408]\n",
      "[epoch#4/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:49<00:00, 36.30s/it, batch loss=0.04345, loss=0.03493]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9579, mse=0.0343]\n",
      "[epoch#5/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:37<00:00, 36.16s/it, batch loss=0.00144, loss=0.03183]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9684, mse=0.0309]\n",
      "[epoch#6/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:58<00:00, 36.41s/it, batch loss=0.01932, loss=0.02936]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9684, mse=0.0276]\n",
      "[epoch#7/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:31<00:00, 36.08s/it, batch loss=0.04917, loss=0.02808]\n",
      "100%|███████████████████| 95/95 [06:20<00:00,  4.00s/it, acc=0.9579, mse=0.0267]\n",
      "[epoch#8/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:02<00:00, 35.74s/it, batch loss=0.04060, loss=0.02672]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9474, mse=0.0256]\n",
      "[epoch#9/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [51:19<00:00, 36.66s/it, batch loss=0.00896, loss=0.02552]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9579, mse=0.0241]\n",
      "[epoch#10/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:47<00:00, 36.28s/it, batch loss=0.03088, loss=0.02468]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9474, mse=0.0234]\n",
      "[epoch#11/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:31<00:00, 36.08s/it, batch loss=0.00436, loss=0.02400]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9579, mse=0.0233]\n",
      "[epoch#12/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:56<00:00, 36.39s/it, batch loss=0.01185, loss=0.02339]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9579, mse=0.0235]\n",
      "[epoch#13/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:32<00:00, 36.10s/it, batch loss=0.01118, loss=0.02278]\n",
      "100%|███████████████████| 95/95 [06:20<00:00,  4.00s/it, acc=0.9579, mse=0.0222]\n",
      "[epoch#14/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:26<00:00, 36.03s/it, batch loss=0.00442, loss=0.02220]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9579, mse=0.0221]\n",
      "[epoch#15/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [51:42<00:00, 36.93s/it, batch loss=0.00519, loss=0.02211]\n",
      "100%|███████████████████| 95/95 [06:21<00:00,  4.02s/it, acc=0.9474, mse=0.0221]\n",
      "[epoch#16/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:16<00:00, 35.91s/it, batch loss=0.02158, loss=0.02150]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9579, mse=0.0214]\n",
      "[epoch#17/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:41<00:00, 36.21s/it, batch loss=0.01491, loss=0.02101]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  4.00s/it, acc=0.9579, mse=0.0216]\n",
      "[epoch#18/50][2656/2660]: 100%|█████████████████████████████████| 84/84 [50:46<00:00, 36.26s/it, batch loss=0.02162, loss=0.02081]\n",
      "100%|███████████████████| 95/95 [06:19<00:00,  3.99s/it, acc=0.9579, mse=0.0206]\n",
      "[epoch#19/50][800/2660]:  30%|██████████                        | 25/84 [15:34<36:46, 37.40s/it, batch loss=0.01658, loss=0.02073]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/public/home/cs177h/lianyh/perl5/project/explore copy.ipynb Cell 74\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/lianyh/perl5/project/explore%20copy.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m bar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[epoch#\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHES\u001b[39m}\u001b[39;00m\u001b[39m][\u001b[39m\u001b[39m{\u001b[39;00mbatch \u001b[39m*\u001b[39m BATCH_SIZE\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_dataset)\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/lianyh/perl5/project/explore%20copy.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m x, y \u001b[39m=\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mmsa\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcuda(non_blocking \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m), sample[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/lianyh/perl5/project/explore%20copy.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/lianyh/perl5/project/explore%20copy.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.15.89.191/public/home/cs177h/lianyh/perl5/project/explore%20copy.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:1073\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1074\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1075\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(checkpoint_epoch + 1, EPOCHES + 1):\n",
    "    losses, tot = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    with tqdm(total= len(train_loader), ncols=130) as bar:\n",
    "        for batch, sample in enumerate(train_loader) :\n",
    "            bar.set_description(f\"[epoch#{epoch}/{EPOCHES}][{batch * BATCH_SIZE}/{len(train_dataset)}]\")\n",
    "\n",
    "            x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses += loss.item() * x.size(0)\n",
    "            tot += x.size(0)\n",
    "            bar.set_postfix({\n",
    "                \"batch loss\" : f\"{loss.item():.5f}\",\n",
    "                \"loss\": f\"{losses / tot:.5f}\"\n",
    "            })\n",
    "            bar.update(1)\n",
    "\n",
    "    test_metric =  calc_test_metric(model)\n",
    "    train_mse, test_acc, valid_mse =  losses / tot, test_metric['accuracy'], test_metric['mse']\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': (model.module if USE_PARALLEL else model).state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_mse': train_mse,\n",
    "        'valid_mse': valid_mse,\n",
    "        'test_acc': test_acc\n",
    "    }, MODEL_PATH / f\"model_{model_name}_epoch{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1026-D1_rand13_fm of 202 sequences is scored 16.78.\n",
      "T1026-D1_rosetta_fm of 19 sequences is scored 93.325.\n",
      "------------\n",
      "T1030-D2_base_fm of 27 sequences is scored 91.177.\n",
      "T1030-D2_original_fm of 256 sequences is scored 60.085.\n",
      "------------\n",
      "T1033-D1_base_fm of 11 sequences is scored 85.25.\n",
      "T1033-D1_rand13_fm of 209 sequences is scored 36.0.\n",
      "------------\n",
      "T1036s1-D1_our_fm of 256 sequences is scored 36.192.\n",
      "T1036s1-D1_rand19_fm of 200 sequences is scored 65.053.\n",
      "------------\n",
      "T1038-D1_base_fm of 110 sequences is scored 89.475.\n",
      "T1038-D1_rand6_fm of 207 sequences is scored 16.23.\n",
      "------------\n",
      "T1038-D2_base_fm of 70 sequences is scored 91.778.\n",
      "T1038-D2_rand13_fm of 200 sequences is scored 70.725.\n",
      "------------\n",
      "T1046s1-D1_rand13_fm of 204 sequences is scored 63.542.\n",
      "T1046s1-D1_rosetta_fm of 44 sequences is scored 97.915.\n",
      "------------\n",
      "T1046s2-D1_rand1_fm of 201 sequences is scored 21.633.\n",
      "T1046s2-D1_rosetta_fm of 52 sequences is scored 98.935.\n",
      "------------\n",
      "T1047s2-D2_rand16_fm of 204 sequences is scored 75.3.\n",
      "T1047s2-D2_rand5_fm of 202 sequences is scored 93.978.\n",
      "------------\n",
      "T1053-D1_base_fm of 256 sequences is scored 95.418.\n",
      "T1053-D1_our_fm of 256 sequences is scored 72.635.\n",
      "------------\n",
      "T1053-D2_rand13_fm of 205 sequences is scored 74.707.\n",
      "T1053-D2_rosetta_fm of 62 sequences is scored 84.065.\n",
      "------------\n",
      "T1061-D1_cov50_fm of 256 sequences is scored 77.37.\n",
      "T1061-D1_original_fm of 256 sequences is scored 55.172.\n",
      "------------\n",
      "T1062-D1_rand13_fm of 201 sequences is scored 81.43.\n",
      "T1062-D1_rosetta_fm of 18 sequences is scored 88.57.\n",
      "------------\n",
      "T1064-D1_base_fm of 48 sequences is scored 66.302.\n",
      "T1064-D1_rand5_fm of 200 sequences is scored 20.38.\n",
      "------------\n",
      "T1068-D1_our_fm of 256 sequences is scored 86.733.\n",
      "T1068-D1_rosetta_fm of 32 sequences is scored 97.207.\n",
      "------------\n",
      "T1070-D3_rand13_fm of 209 sequences is scored 73.685.\n",
      "T1070-D3_rand16_fm of 208 sequences is scored 83.225.\n",
      "------------\n",
      "T1073-D1_rand12_fm of 200 sequences is scored 92.797.\n",
      "T1073-D1_rand13_fm of 203 sequences is scored 89.41.\n",
      "------------\n",
      "T1082-D1_aug_fm of 126 sequences is scored 40.0.\n",
      "T1082-D1_rosetta_fm of 11 sequences is scored 96.668.\n",
      "------------\n",
      "T1093-D3_base_fm of 44 sequences is scored 92.69.\n",
      "T1093-D3_rand13_fm of 201 sequences is scored 22.17.\n",
      "------------\n",
      "T1099-D1_cov50_fm of 256 sequences is scored 27.527.\n",
      "T1099-D1_rosetta_fm of 8 sequences is scored 80.058.\n",
      "------------\n",
      "T1101-D1_rand13_fm of 203 sequences is scored 97.892.\n",
      "T1101-D1_rand18_fm of 202 sequences is scored 99.398.\n",
      "------------\n",
      "0.7789473684210526\n"
     ]
    }
   ],
   "source": [
    "# Baseline: output the one with more sequences\n",
    "def calc_baseline_test_acc(test_dataset):\n",
    "    correct = 0\n",
    "    pairs = len(test_dataset) // 2\n",
    "    for i in range(pairs) :\n",
    "        msa1, msa2 = test_dataset[2*i], test_dataset[2*i+1]\n",
    "        pred = len(msa1['msa']) > len(msa2['msa'])\n",
    "        gt = msa1['score'] > msa2['score']\n",
    "        if pred == gt : \n",
    "            correct += 1\n",
    "        else :\n",
    "            print(f\"{msa1['name']} of {len(msa1['msa'])} sequences is scored {msa1['score']}.\")\n",
    "            print(f\"{msa2['name']} of {len(msa2['msa'])} sequences is scored {msa2['score']}.\")\n",
    "            print(\"------------\")\n",
    "    return correct / pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256x 155 y_gt:0.8572  y_pred:79.3215                                           \n",
      " 203x 155 y_gt:0.8149  y_pred:85.1131                                           \n",
      "-------                                                                         \n",
      "  11x 101 y_gt:0.8525  y_pred:49.7983                                           \n",
      " 256x 101 y_gt:0.3600  y_pred:54.4962                                           \n",
      "-------                                                                         \n",
      " 256x 350 y_gt:0.9542  y_pred:80.1949                                           \n",
      " 256x 350 y_gt:0.7264  y_pred:82.2742                                           \n",
      "-------                                                                         \n",
      " 200x  60 y_gt:0.9280  y_pred:82.2290                                           \n",
      " 256x  60 y_gt:0.8941  y_pred:82.4489                                           \n",
      "-------                                                                         \n",
      "100%|███████████████████| 95/95 [06:20<00:00,  4.01s/it, acc=0.9579, mse=0.0173]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9578947368421052, 'mse': 0.01726992030514674}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_predictions(model, only_print_wrong = True):\n",
    "    model.eval()\n",
    "    mse, correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total = len(test_loader), ncols=80, file=sys.stdout) as bar:\n",
    "            for sample in test_loader :\n",
    "                x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "                pred = model(x)\n",
    "\n",
    "                mse += torch.sum((y - pred) ** 2).item() \n",
    "                if torch.argmax(y) == torch.argmax(pred) :\n",
    "                    correct += 1\n",
    "                    if not only_print_wrong:\n",
    "                        for x, y_gt, y_pred in zip(x, y, pred) :\n",
    "                            tqdm.write(f\"{list(x[:, 0]).index(1) if 1 in x[:, 0] else 256:>4d}x{x.size(1):>4d} y_gt:{y_gt.item():.4f}  y_pred:{y_pred.item() * 100:.4f}\")\n",
    "                        tqdm.write(\"======\")\n",
    "                else :\n",
    "                    for x, y_gt, y_pred in zip(x, y, pred) :\n",
    "                        tqdm.write(f\"{list(x[:, 0]).index(1) if 1 in x[:, 0] else 256:>4d}x{x.size(1):>4d} y_gt:{y_gt.item():.4f}  y_pred:{y_pred.item() * 100:.4f}\")\n",
    "                    tqdm.write(\"-------\")\n",
    "                tot += 2\n",
    "\n",
    "                bar.set_postfix({\n",
    "                    \"acc\" : f\"{correct / (tot // 2):.4f}\",\n",
    "                    \"mse\": f\"{mse / tot:.4f}\"\n",
    "                })\n",
    "                bar.update(1)\n",
    "    \n",
    "    return {'accuracy' : correct / (tot // 2), 'mse' : mse / tot}\n",
    "\n",
    "print_predictions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 18/18 [00:34<00:00,  1.93s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18
         ],
         "xaxis": "x",
         "y": [
          0.057782340834015296,
          0.04882057510260353,
          0.04081329990150337,
          0.03493036032843411,
          0.0318323787856721,
          0.02935772131204157,
          0.02807894866717489,
          0.026718181646183916,
          0.02552013031550144,
          0.024676883164653204,
          0.023996117467382796,
          0.02338980601582312,
          0.022782472378217188,
          0.02220341877913789,
          0.022106580993622765,
          0.021504528058650798,
          0.021009477760110584,
          0.02081390372232387
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "Valid MSE",
         "type": "scatter",
         "x": [
          1.5,
          2.5,
          3.5,
          4.5,
          5.5,
          6.5,
          7.5,
          8.5,
          9.5,
          10.5,
          11.5,
          12.5,
          13.5,
          14.5,
          15.5,
          16.5,
          17.5,
          18.5
         ],
         "xaxis": "x",
         "y": [
          0.067524799569755,
          0.0519751360750218,
          0.040777996477769,
          0.034281415717774315,
          0.030936399774356305,
          0.02755971539223346,
          0.026734920540567194,
          0.025634487177318845,
          0.024109007763304123,
          0.02335468087656313,
          0.023286028765141964,
          0.023496420659806685,
          0.022243578100843528,
          0.022082887290162036,
          0.0221217307210808,
          0.021387076759435186,
          0.021621268723270318,
          0.02062851033253154
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "Test Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18
         ],
         "xaxis": "x",
         "y": [
          0.8947368421052632,
          0.9368421052631579,
          0.9578947368421052,
          0.9578947368421052,
          0.968421052631579,
          0.968421052631579,
          0.9578947368421052,
          0.9473684210526315,
          0.9578947368421052,
          0.9473684210526315,
          0.9578947368421052,
          0.9578947368421052,
          0.9578947368421052,
          0.9578947368421052,
          0.9473684210526315,
          0.9578947368421052,
          0.9578947368421052,
          0.9578947368421052
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "size": 12
         },
         "x": 0.75,
         "y": 0.45
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "MSE Loss"
         }
        },
        "yaxis2": {
         "overlaying": "y",
         "range": [
          0.85,
          1
         ],
         "side": "right",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c1bcc7e8-8d05-48cd-a3ad-34127761a07c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c1bcc7e8-8d05-48cd-a3ad-34127761a07c\")) {                    Plotly.newPlot(                        \"c1bcc7e8-8d05-48cd-a3ad-34127761a07c\",                        [{\"mode\":\"lines+markers\",\"name\":\"Train Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\"xaxis\":\"x\",\"y\":[0.057782340834015296,0.04882057510260353,0.04081329990150337,0.03493036032843411,0.0318323787856721,0.02935772131204157,0.02807894866717489,0.026718181646183916,0.02552013031550144,0.024676883164653204,0.023996117467382796,0.02338980601582312,0.022782472378217188,0.02220341877913789,0.022106580993622765,0.021504528058650798,0.021009477760110584,0.02081390372232387],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Valid MSE\",\"x\":[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5,12.5,13.5,14.5,15.5,16.5,17.5,18.5],\"xaxis\":\"x\",\"y\":[0.067524799569755,0.0519751360750218,0.040777996477769,0.034281415717774315,0.030936399774356305,0.02755971539223346,0.026734920540567194,0.025634487177318845,0.024109007763304123,0.02335468087656313,0.023286028765141964,0.023496420659806685,0.022243578100843528,0.022082887290162036,0.0221217307210808,0.021387076759435186,0.021621268723270318,0.02062851033253154],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Test Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\"xaxis\":\"x\",\"y\":[0.8947368421052632,0.9368421052631579,0.9578947368421052,0.9578947368421052,0.968421052631579,0.968421052631579,0.9578947368421052,0.9473684210526315,0.9578947368421052,0.9473684210526315,0.9578947368421052,0.9578947368421052,0.9578947368421052,0.9578947368421052,0.9473684210526315,0.9578947368421052,0.9578947368421052,0.9578947368421052],\"yaxis\":\"y2\",\"type\":\"scatter\"}],                        {\"legend\":{\"font\":{\"color\":\"black\",\"size\":12},\"x\":0.75,\"y\":0.45},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"MSE Loss\"}},\"yaxis2\":{\"overlaying\":\"y\",\"range\":[0.85,1.0],\"side\":\"right\",\"title\":{\"text\":\"Accuracy\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c1bcc7e8-8d05-48cd-a3ad-34127761a07c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_history(model_name = \"mean_256\", trained_epoches = 10):\n",
    "    epoches = list(range(1, trained_epoches + 1))\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    for epoch in tqdm(range(1, trained_epoches+1)):\n",
    "        checkpoint = torch.load(MODEL_PATH / f\"model_{model_name}_epoch{epoch}.pth\")\n",
    "        train_losses.append(checkpoint[\"train_mse\"])\n",
    "        test_losses.append(checkpoint[\"valid_mse\"])\n",
    "        test_accs.append(checkpoint[\"test_acc\"])\n",
    "    \n",
    "    trace1 = go.Scatter(\n",
    "        x = epoches,\n",
    "        y = train_losses,\n",
    "        name= \"Train Loss\",\n",
    "        xaxis='x',\n",
    "        yaxis='y1',\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = [epoch + 0.5 for epoch in epoches],\n",
    "        y = test_losses,\n",
    "        name= \"Valid MSE\",\n",
    "        xaxis='x', \n",
    "        yaxis='y1',\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x = epoches,\n",
    "        y = test_accs,\n",
    "        name='Test Accuracy',\n",
    "        xaxis='x', \n",
    "        yaxis='y2',\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "    \n",
    "    data = [trace1, trace2, trace3]\n",
    "    layout = go.Layout(\n",
    "        yaxis2=dict(overlaying = 'y', side = 'right', title = \"Accuracy\", range = [0.85, 1.0]),\n",
    "        yaxis1=dict(title = \"MSE Loss\"),\n",
    "        xaxis = dict(title = \"Epoch\"),\n",
    "        legend=dict(x=0.75, y=0.45, font=dict(size=12, color=\"black\"))\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "plot_training_history(\"bos_256\", 18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e0b1f18ae03a7035bc033a9f9c7ae76394b92e608ff6812adcccfdadc13d2c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
