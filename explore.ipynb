{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sinfo -O Nodehost,Gres:.30,GresUsed:.45\n",
    "!salloc -N 1 --cpus-per-task=4 -p CS177h --gres=gpu:TeslaM4024GB:1\n",
    "!salloc -N 1 --cpus-per-task=8 -p CS177h --gres=gpu:TeslaM4024GB:2\n",
    "!salloc -N 1 --cpus-per-task=12 -p CS177h --gres=gpu:TeslaM4024GB:3\n",
    "!salloc -N 1 --cpus-per-task=16 -p CS177h --gres=gpu:TeslaM4024GB:4\n",
    "!jupyter-lab --no-brows --ip=0.0.0.0 --port=7774 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fair-esm\n",
    "%conda install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 31 19:05:48 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M40 24GB      On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   28C    P8    28W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla M40 24GB      On   | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   28C    P8    27W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla M40 24GB      On   | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   28C    P8    28W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla M40 24GB      On   | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   28C    P8    27W / 250W |      0MiB / 22945MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/public/home/cs177h/lianyh/perl5/project\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import esm\n",
    "\n",
    "DATASET_PATH = Path() / \"perl5\" / \"project\" / \"dataset\" / \"CASP14_fm\"\n",
    "MODEL_PATH = Path() / \"perl5\" / \"project\" / \"model\"\n",
    "EMBDEDDINGS_PATH = Path() / \"perl5\" / \"project\" / \"embdeddings\"\n",
    "TRANSFORMER_PATH = Path() / \"perl5\" / \"project\" / \"esm_msa1b_t12_100M_UR50S.pt\"\n",
    "\n",
    "# hyperparameters\n",
    "MAX_DEPTH = 256\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.a3m` MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T1024-D1:',\n",
       "  'KEFWNLDKNLQLRLGIVFLGAFSYGTVFSSMTIYYNQYLGSAITGILLALSAVATFVAGILAGFFADRNGRKPVMVFGTIIQLLGAALAIASNLPGHVNPWSTFIAFLLISFGYNFVITAGNAMIIDASNAENRKVVFMLDYWAQNLSVILGAALGAWLFRPAFEALLVILLLTVLVSFFLTTFVMTETFKPT'),\n",
       " ('3799131 len:525',\n",
       "  'KGLWALAANTGERFGYTMLAVF---LLYLQANFHYEAGLASTIYSTFLML----VYFLPVIGGIAADRFGFGKMVTTGIFIMFIGYLL-LSIPMGGDYMAIIAMLALILVSLGTGLF--KGNLQVMAPQYADKRDSGFSLFYMAINIGAMFAPTIMDWYYHFAFAVACVSLIVSILIYYFTSSTYN-------'),\n",
       " ('tr|A0A0R1VVR4|A0A0R1VVR4_9LACO Major facilitator superfamily permease OS=Lactobacillus ghanensis DSM 18630 GN=FC89_GL000394 PE=4 SV=1',\n",
       "  'ENIMK--KNLPILLA-SMLVNMGIGLIMPITTLFLHNRLHQTLVTVLMGF-SLAMVLGNLLGGWLFDHWKVKPTHYLGGLLVWLNLTLLIIFPI------WPLY-TILVIGYGFGL----GILNSAIAAHQKKSPNLFTNAYWLANLGMGLATF----NIRWVFSVALFIFIVTLLVVFFHE-----------'),\n",
       " ('8253022 len:434',\n",
       "  'ANFILLV------LG-QGISLFGNTMLFAMSMWVLDETASSTTFATVLAISVIPTILISPFGGVMADRVSKRAMMVISGIVTLLATVF---FALSG----FNILIAIMQVVLAVLDAMETPVVQSAGRESSTDLRRGAAIINQVQQLSQLLPSFLGGVGIRPMMLITAACLMSAAMVECFIRLAKPNQ-----'),\n",
       " ('7093751 len:409',\n",
       "  'KDIRQTDRKIIILLT-VLVDVLGVGIVIPILPFYEENGVSPLVLTLLIAVFSFFSFISAPMLGALSDKIGRRPVLIISIASTAIGWLV---FAWANSI--WMLFLGRIIDGMA------AGNLPVAIAKDEKERTKNLGLISAVFGVGFIIGPALGA-ITLPFFVVGFMALLNTLAAIIFLPETNL-------')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa = read_msa(DATASET_PATH / \"train\" / \"T1024-D1_aug_fm.a3m\")\n",
    "msa[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MSAs\n",
    "\n",
    "read MSA from `.a3m` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_msa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_data_train, _ = read_msa_data(root = DATASET_PATH, is_train = True)\n",
    "msa_data_test, _ = read_msa_data(root = DATASET_PATH, is_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train MSAs: 2660\n",
      "> max MSA depth in train set: 32494\n",
      "> max sequence length in train set: 583\n",
      "# of test MSAs: 190\n",
      "> max MSA depth in test set: 21298\n",
      "> max sequence length in test set: 583\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of train MSAs: {len(msa_data_train)}\")\n",
    "print(f\"> max MSA depth in train set: {max(len(msa['msa']) for _, msa in msa_data_train.items())}\" )\n",
    "print(f\"> max sequence length in train set: {max(len(msa['msa'][0][1]) for _, msa in msa_data_train.items())}\" )\n",
    "\n",
    "print(f\"# of test MSAs: {len(msa_data_test)}\")\n",
    "print(f\"> max MSA depth in test set: {max(len(msa['msa']) for _, msa in msa_data_test.items())}\" )\n",
    "print(f\"> max sequence length in test set: {max(len(msa['msa'][0][1]) for _, msa in msa_data_test.items())}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort MSAs by scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_scored_msas(msa_data, top_k = 10, bot_k = 5) :\n",
    "    msa_scores = sorted([(msa['score'], msa_name) for msa_name, msa in msa_data.items()], reverse = True)\n",
    "    for _, msa_name in msa_scores[:top_k] :\n",
    "        msa = msa_data[msa_name]\n",
    "        print(f\"MSA {msa_name} has { len(msa['msa']) } sequences of length { len(msa['msa'][0][1]) }, scored {msa['score']:.3f}\")\n",
    "    print(\"......\")\n",
    "    for _, msa_name in msa_scores[-bot_k:] :\n",
    "        msa = msa_data[msa_name]\n",
    "        print(f\"MSA {msa_name} has { len(msa['msa']) } sequences of length { len(msa['msa'][0][1]) }, scored {msa['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_rosetta_fm has 10403 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_our_fm has 4277 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_original_fm has 8885 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_meta_fm has 6439 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_deduplicated_fm has 2155 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_cov50_fm has 1956 sequences of length 101, scored 99.505\n",
      "MSA T1101-D1_rand22_fm has 204 sequences of length 83, scored 99.097\n",
      "MSA T1065s2-D1_original_fm has 8868 sequences of length 98, scored 98.980\n",
      "MSA T1101-D1_rand9_fm has 203 sequences of length 83, scored 98.795\n",
      "MSA T1101-D1_rand19_fm has 203 sequences of length 83, scored 98.795\n",
      "......\n",
      "MSA T1026-D1_rand9_fm has 201 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand15_fm has 206 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand7_fm has 200 sequences of length 146, scored 15.755\n",
      "MSA T1043-D1_rand3_fm has 204 sequences of length 148, scored 15.370\n",
      "MSA T1028-D1_rand5_fm has 208 sequences of length 292, scored 13.697\n"
     ]
    }
   ],
   "source": [
    "print_top_scored_msas(msa_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_base_fm has 4808 sequences of length 101, scored 99.752\n",
      "MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "MSA T1065s2-D1_base_fm has 4543 sequences of length 98, scored 99.235\n",
      "MSA T1046s2-D1_rosetta_fm has 52 sequences of length 141, scored 98.935\n",
      "MSA T1070-D4_base_fm has 398 sequences of length 68, scored 98.162\n",
      "MSA T1091-D4_our_fm has 4888 sequences of length 112, scored 97.990\n",
      "MSA T1046s1-D1_rosetta_fm has 44 sequences of length 72, scored 97.915\n",
      "MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "MSA T1052-D3_original_fm has 3201 sequences of length 80, scored 97.812\n",
      "MSA T1089-D1_meta_fm has 9576 sequences of length 377, scored 97.810\n",
      "......\n",
      "MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "MSA T1056-D1_rand15_fm has 203 sequences of length 169, scored 16.273\n",
      "MSA T1038-D1_rand6_fm has 207 sequences of length 114, scored 16.230\n",
      "MSA T1090-D1_rand6_fm has 201 sequences of length 189, scored 15.475\n",
      "MSA T1043-D1_rand1_fm has 200 sequences of length 148, scored 14.525\n"
     ]
    }
   ],
   "source": [
    "print_top_scored_msas(msa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH',\n",
       " '----IEHHHKAAEHHEHAAKHHHAAAEHHQNGDHEKASHHAHAAHGHALHAEHHASEAAKHHANEHG----',\n",
       " '-HKGAEHHHKAAEHHEHAARHHREAAKHYETGNHEKAAHHAHVAHGHHLHARHHAEEATKHHASEHG----',\n",
       " '-HKGAEHHKKAAEHHELAAKHHREAAKHHEAGSHEKGAHHSEIAAGHGLHATYHTEEATKHHAEEHTG---',\n",
       " '--QAAEHHRKAAEHHEHAARHHEEAAEHHEAGKHETAAHHAHLARAHHEVATHHAVEAAKAHLEQHG----']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa = msa_data_test[\"T1084-D1_our_fm\"]['msa']\n",
    "msa_sequences = [sequence for _, sequence in msa]\n",
    "msa_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences (depth) : 2823\n",
      "# of protetins in a sequence: 71\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of sequences (depth) : { len(msa_sequences) }\")\n",
    "print(f\"# of protetins in a sequence: { len(msa_sequences[0]) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sequences in an MSA share the same number of protetins.\n"
     ]
    }
   ],
   "source": [
    "def same_length(msa_data):\n",
    "    return all(all([len(sequence) == len(msa['msa'][0]) for sequence in msa['msa']]) for msa in msa_data.values())\n",
    "\n",
    "if same_length(msa_data_test) and same_length(msa_data_train):\n",
    "    print(\"All sequences in an MSA share the same number of protetins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling MSA\n",
    "\n",
    "Subsampling MSA = swallowing MSA depth = decreasing # of sequences in an MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HH Filter\n",
    "\n",
    "We can see the number of sequences (also called MSA depth) of MSAs varies and some are very deep. Furthermore, the MSA Transformer only supports maximum MSA depth $\\leq 1024$. \n",
    "\n",
    "We try to cut the number of sequences of deeper MSAs to 256 each. A simple approach is to intercept the first 256 sequences. However, let's try the method called **HH Filter** with the `-diff 256` parameter, which can extract a representative set (256 or more, usually close to 256) of the sequences from an alignment. If more than 256 sequences are returned we apply the Diversity Maximizing strategy on the HHFilter output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge -c bioconda hhsuite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run HHFilter in command line like `hhfilter -i <INTPUT> -o <OUTPUT> [options] `. See https://manpages.ubuntu.com/manpages/bionic/man1/hhfilter.1.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 15:47:19.878 INFO: Input file = perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m\n",
      "\n",
      "- 15:47:19.878 INFO: Output file = perl5/project/dataset/CASP14_fm/test_filtered/T1084-D1_our_fm.a3m\n",
      "\n",
      "- 15:47:19.990 DEBUG: Read perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m with 2823 sequences\n",
      "\n",
      "- 15:47:19.990 DEBUG: Single sequence in file perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m contains only 71 match_states! Switching to option -M first\n",
      " seq= AHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH\n",
      "\n",
      "- 15:47:19.993 DEBUG: Alignment in perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m contains 71 match states\n",
      "\n",
      "- 15:47:19.993 DEBUG: Using the Gonnet matrix\n",
      "\n",
      "- 15:47:19.993 DEBUG: sequence identity = 17.3066 %; entropy per column = 3.93717 bits (out of 4.18529); mutual information = 0.248121 bits\n",
      "\n",
      "- 15:47:20.001 DEBUG: 281 out of 2823 sequences passed filter (\n",
      "- 15:47:20.001 DEBUG: up to 25% position-dependent max pairwise sequence identity)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hhfilter -i perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m -o perl5/project/dataset/CASP14_fm/test_filtered/T1084-D1_our_fm.a3m -diff 256 -v 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do this via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- 15:53:50.158 INFO: Input file = perl5/project/dataset/CASP14_fm/test/T1084-D1_our_fm.a3m\n",
      "\n",
      "- 15:53:50.158 INFO: Output file = perl5/project/dataset/CASP14_fm/test_filtered/T1084-D1_our_fm.a3m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subsampling import hh_filter\n",
    "hh_filter(\"T1084-D1_our_fm.a3m\", DATASET_PATH / \"test\", DATASET_PATH / \"test_filtered\", diff=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see 281 (close to 256) sequences are filtered out of all 2823 sequences. Now let's apply for all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsampling import filter_msa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2660/2660 [01:30<00:00, 29.43it/s]\n"
     ]
    }
   ],
   "source": [
    "filter_msa_data(msa_data_train, root = DATASET_PATH, is_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:15<00:00, 12.24it/s]\n"
     ]
    }
   ],
   "source": [
    "filter_msa_data(msa_data_test, root = DATASET_PATH, is_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see MSA depths are much better but some are still $>256$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_rosetta_fm has 267 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_our_fm has 285 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_original_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_meta_fm has 485 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_deduplicated_fm has 437 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_cov50_fm has 388 sequences of length 101, scored 99.505\n",
      "MSA T1101-D1_rand22_fm has 204 sequences of length 83, scored 99.097\n",
      "MSA T1065s2-D1_original_fm has 314 sequences of length 98, scored 98.980\n",
      "MSA T1101-D1_rand9_fm has 203 sequences of length 83, scored 98.795\n",
      "MSA T1101-D1_rand19_fm has 203 sequences of length 83, scored 98.795\n",
      "......\n",
      "MSA T1026-D1_rand9_fm has 201 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand15_fm has 206 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand7_fm has 200 sequences of length 146, scored 15.755\n",
      "MSA T1043-D1_rand3_fm has 204 sequences of length 148, scored 15.370\n",
      "MSA T1028-D1_rand5_fm has 208 sequences of length 292, scored 13.697\n"
     ]
    }
   ],
   "source": [
    "msa_data_train, _ = read_msa_data(is_train = True, is_filtered = True)\n",
    "print_top_scored_msas(msa_data_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_base_fm has 420 sequences of length 101, scored 99.752\n",
      "MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "MSA T1065s2-D1_base_fm has 331 sequences of length 98, scored 99.235\n",
      "MSA T1046s2-D1_rosetta_fm has 52 sequences of length 141, scored 98.935\n",
      "MSA T1070-D4_base_fm has 227 sequences of length 68, scored 98.162\n",
      "MSA T1091-D4_our_fm has 556 sequences of length 112, scored 97.990\n",
      "MSA T1046s1-D1_rosetta_fm has 44 sequences of length 72, scored 97.915\n",
      "MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "MSA T1052-D3_original_fm has 273 sequences of length 80, scored 97.812\n",
      "MSA T1089-D1_meta_fm has 455 sequences of length 377, scored 97.810\n",
      "......\n",
      "MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "MSA T1056-D1_rand15_fm has 203 sequences of length 169, scored 16.273\n",
      "MSA T1038-D1_rand6_fm has 207 sequences of length 114, scored 16.230\n",
      "MSA T1090-D1_rand6_fm has 201 sequences of length 189, scored 15.475\n",
      "MSA T1043-D1_rand1_fm has 200 sequences of length 148, scored 14.525\n"
     ]
    }
   ],
   "source": [
    "msa_data_test, _ = read_msa_data(is_train = False, is_filtered = True)\n",
    "print_top_scored_msas(msa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity Maxmizing\n",
    "\n",
    "Diversity Maximizing is a greedy strategy which starts from the reference and adds the sequence with highest average hamming distance to current set of sequences. Hence we can cut each MSA to at most 256 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsampling import greedy_subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before subsampling: 281\n"
     ]
    }
   ],
   "source": [
    "msa = read_msa(DATASET_PATH / \"test_filtered\" / \"T1084-D1_our_fm.a3m\")\n",
    "print(f\"before subsampling: { len(msa) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after subsampling: 256\n"
     ]
    }
   ],
   "source": [
    "msa_subsampled = greedy_subsampling(msa)\n",
    "print(f\"after subsampling: { len(msa_subsampled) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply for all MSAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsampling import subsampling_msa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_rosetta_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_our_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_original_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1070-D2_meta_fm has 256 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_deduplicated_fm has 256 sequences of length 101, scored 99.505\n",
      "MSA T1070-D2_cov50_fm has 256 sequences of length 101, scored 99.505\n",
      "MSA T1101-D1_rand22_fm has 204 sequences of length 83, scored 99.097\n",
      "MSA T1065s2-D1_original_fm has 256 sequences of length 98, scored 98.980\n",
      "MSA T1101-D1_rand9_fm has 203 sequences of length 83, scored 98.795\n",
      "MSA T1101-D1_rand19_fm has 203 sequences of length 83, scored 98.795\n",
      "......\n",
      "MSA T1026-D1_rand9_fm has 201 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand15_fm has 206 sequences of length 146, scored 15.925\n",
      "MSA T1026-D1_rand7_fm has 200 sequences of length 146, scored 15.755\n",
      "MSA T1043-D1_rand3_fm has 204 sequences of length 148, scored 15.370\n",
      "MSA T1028-D1_rand5_fm has 208 sequences of length 292, scored 13.697\n"
     ]
    }
   ],
   "source": [
    "msa_data_train = subsampling_msa_data(msa_data_train)\n",
    "print_top_scored_msas(msa_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA T1070-D2_base_fm has 256 sequences of length 101, scored 99.752\n",
      "MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "MSA T1065s2-D1_base_fm has 256 sequences of length 98, scored 99.235\n",
      "MSA T1046s2-D1_rosetta_fm has 52 sequences of length 141, scored 98.935\n",
      "MSA T1070-D4_base_fm has 227 sequences of length 68, scored 98.162\n",
      "MSA T1091-D4_our_fm has 256 sequences of length 112, scored 97.990\n",
      "MSA T1046s1-D1_rosetta_fm has 44 sequences of length 72, scored 97.915\n",
      "MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "MSA T1052-D3_original_fm has 256 sequences of length 80, scored 97.812\n",
      "MSA T1089-D1_meta_fm has 256 sequences of length 377, scored 97.810\n",
      "......\n",
      "MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "MSA T1056-D1_rand15_fm has 203 sequences of length 169, scored 16.273\n",
      "MSA T1038-D1_rand6_fm has 207 sequences of length 114, scored 16.230\n",
      "MSA T1090-D1_rand6_fm has 201 sequences of length 189, scored 15.475\n",
      "MSA T1043-D1_rand1_fm has 200 sequences of length 148, scored 14.525\n"
     ]
    }
   ],
   "source": [
    "msa_data_test = subsampling_msa_data(msa_data_test)\n",
    "print_top_scored_msas(msa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 2660/2660 [00:39<00:00, 66.53it/s]\n"
     ]
    }
   ],
   "source": [
    "msa_data_train, _ = read_msa_data(is_train = True, is_filtered = True)\n",
    "msa_data_train = subsampling_msa_data(msa_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:06<00:00, 27.93it/s]\n"
     ]
    }
   ],
   "source": [
    "msa_data_test, _ = read_msa_data(is_train = False, is_filtered = True)\n",
    "msa_data_test = subsampling_msa_data(msa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load MSA Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_TRANSFORMER = False\n",
    "if DOWNLOAD_TRANSFORMER :\n",
    "    msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "else :\n",
    "    msa_transformer, msa_alphabet = esm.pretrained.load_model_and_alphabet_local(TRANSFORMER_PATH)\n",
    "\n",
    "# USE_PARALLEL = torch.cuda.device_count() > 1\n",
    "# # USE_PARALLEL = False\n",
    "# if USE_PARALLEL :\n",
    "#     msa_transformer = torch.nn.DataParallel(msa_transformer)\n",
    "    \n",
    "msa_transformer = msa_transformer.eval().cuda()\n",
    "msa_batch_converter = msa_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet size: 33\n",
      "padding token: 1\n",
      "begin of sequence token : 0\n",
      "end of sequence token (not used here) : 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"alphabet size: {len(msa_alphabet)}\")\n",
    "print(f\"padding token: {msa_alphabet.padding_idx}\")\n",
    "print(f\"begin of sequence token : {msa_alphabet.cls_idx}\")\n",
    "print(f\"end of sequence token (not used here) : {msa_alphabet.eos_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSATransformer(\n",
       "  (embed_tokens): Embedding(33, 768, padding_idx=1)\n",
       "  (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (6): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (7): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (9): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (10): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (11): AxialTransformerLayer(\n",
       "      (row_self_attention): NormalizedResidualBlock(\n",
       "        (layer): RowSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (column_self_attention): NormalizedResidualBlock(\n",
       "        (layer): ColumnSelfAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forward_layer): NormalizedResidualBlock(\n",
       "        (layer): FeedForwardNetwork(\n",
       "          (activation_fn): GELU(approximate=none)\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=144, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "  (emb_layer_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (emb_layer_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 267, 194]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from dataset import read_msa\n",
    "msa_data = [\n",
    "    read_msa(DATASET_PATH / \"train_filtered\" / \"T1024-D1_aug_fm.a3m\"),\n",
    "    read_msa(DATASET_PATH / \"train_filtered\" / \"T1024-D1_base_fm.a3m\"),\n",
    "    #read_msa(DATASET_PATH / \"train_filtered\" / \"T1101-D2_rand22_fm.a3m\")\n",
    "]\n",
    "\n",
    "msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(msa_data)\n",
    "print(msa_batch_tokens.shape, msa_batch_tokens.dtype)   # Should be a 3D tensor with dtype torch.int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0, 15,  9,  ..., 15, 14, 11],\n",
      "         [ 0, 15,  6,  ..., 30, 30, 30],\n",
      "         [ 0,  9, 17,  ..., 30, 30, 30],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 15,  9,  ..., 15, 14, 11],\n",
      "         [ 0, 30, 30,  ..., 30, 30, 30],\n",
      "         [ 0, 30, 30,  ..., 30, 11, 30],\n",
      "         ...,\n",
      "         [ 0, 30, 30,  ..., 14, 14, 30],\n",
      "         [ 0, 30, 30,  ..., 10,  5, 30],\n",
      "         [ 0, 30, 30,  ..., 30, 30, 30]]])\n"
     ]
    }
   ],
   "source": [
    "print(msa_batch_tokens)\n",
    "# 0: beginning of sequence\n",
    "# 1: empty\n",
    "# 30: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max MSA depth (# of sequences): 256\n",
      "max length of each sequence: 193\n"
     ]
    }
   ],
   "source": [
    "print(f\"max MSA depth (# of sequences): { msa_batch_tokens.shape[1] }\")\n",
    "print(f\"max length of each sequence: { msa_batch_tokens.shape[2]-1 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of model layers: 12\n",
      "model embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of model layers: {len((msa_transformer.module if USE_PARALLEL else msa_transformer).layers)}\")\n",
    "print(f\"model embedding dimension: {(msa_transformer.module if USE_PARALLEL else msa_transformer).layers[0].embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer.eval()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    results = msa_transformer(msa_batch_tokens.cuda(non_blocking = True), repr_layers=[12])\n",
    "    #results = msa_transformer(msa_batch_tokens.cuda(), repr_layers=[12], need_head_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 214, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = results['representations'][12][:, 0, 1:, :]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15,  9, 18, 22, 17,  4, 13, 15, 17,  4, 16,  4, 10,  4,  6, 12,  7, 18,\n",
      "         4,  6,  5, 18,  8, 19,  6, 11,  7, 18,  8,  8, 20, 11, 12, 19, 19, 17,\n",
      "        16, 19,  4,  6,  8,  5, 12, 11,  6, 12,  4,  4,  5,  4,  8,  5,  7,  5,\n",
      "        11, 18,  7,  5,  6, 12,  4,  5,  6, 18, 18,  5, 13, 10, 17,  6, 10, 15,\n",
      "        14,  7, 20,  7, 18,  6, 11, 12, 12, 16,  4,  4,  6,  5,  5,  4,  5, 12,\n",
      "         5,  8, 17,  4, 14,  6, 21,  7, 17, 14, 22,  8, 11, 18, 12,  5, 18,  4,\n",
      "         4, 12,  8, 18,  6, 19, 17, 18,  7, 12, 11,  5,  6, 17,  5, 20, 12, 12,\n",
      "        13,  5,  8, 17,  5,  9, 17, 10, 15,  7,  7, 18, 20,  4, 13, 19, 22,  5,\n",
      "        16, 17,  4,  8,  7, 12,  4,  6,  5,  5,  4,  6,  5, 22,  4, 18, 10, 14,\n",
      "         5, 18,  9,  5,  4,  4,  7, 12,  4,  4,  4, 11,  7,  4,  7,  8, 18, 18,\n",
      "         4, 11, 11, 18,  7, 20, 11,  9, 11, 18, 15, 14, 11])\n",
      "tensor([15,  9, 18, 22, 17,  4, 13, 15, 17,  4, 16,  4, 10,  4,  6, 12,  7, 18,\n",
      "         4,  6,  5, 18,  8, 19,  6, 11,  7, 18,  8,  8, 20, 11, 12, 19, 19, 17,\n",
      "        16,  6,  4,  6,  8,  5, 12, 11,  6, 12,  4,  4,  5,  4,  8,  5,  7,  5,\n",
      "        11, 18,  7,  5,  6, 12,  4,  5,  6, 18, 18,  5, 13, 10, 18,  6, 10, 15,\n",
      "        14,  7, 20,  7, 18,  6, 11, 12, 12, 16,  4,  4,  6,  5,  5,  4,  5, 12,\n",
      "         5, 18, 14,  4, 14,  6, 21,  7, 17, 14, 22,  8, 11, 18, 12,  5, 18,  4,\n",
      "         4, 12,  8, 18,  6, 19,  6, 18,  7, 12, 11,  5,  6, 17,  5, 16, 12, 12,\n",
      "        13,  5,  8, 17,  5,  9, 17, 10, 15,  7,  7, 18,  8,  4, 13, 19, 22,  5,\n",
      "        16, 17,  4,  8,  7, 12,  4,  6,  5,  5,  4,  6,  5, 22,  4, 18, 10, 14,\n",
      "         5, 18, 12,  5,  6,  4,  7, 12,  4,  4,  4, 11,  7,  4,  7,  8, 18, 18,\n",
      "         4, 11, 11, 15, 15, 15, 11,  9, 11, 18, 15, 14, 11], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(msa_batch_tokens[0][0][1:])\n",
    "\n",
    "print(torch.argmax(results['logits'][0][0][1:], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MSAScoreDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                       | 1876/2660 [00:27<00:16, 46.93it/s]"
     ]
    }
   ],
   "source": [
    "train_dataset = MSAScoreDataset(root = DATASET_PATH, is_train = True)\n",
    "train_dataset.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:06<00:00, 29.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 190 MSAs:\n",
      "> MSA T1024-D1_meta_fm has 256 sequences of length 193, scored 96.243\n",
      "> MSA T1024-D1_rand10_fm has 206 sequences of length 193, scored 56.865\n",
      "> MSA T1024-D2_original_fm has 256 sequences of length 198, scored 95.580\n",
      "> MSA T1024-D2_rand13_fm has 203 sequences of length 198, scored 92.803\n",
      "> MSA T1025-D1_our_fm has 256 sequences of length 257, scored 92.315\n",
      "> MSA T1025-D1_rand1_fm has 202 sequences of length 257, scored 57.297\n",
      "> MSA T1026-D1_rand13_fm has 202 sequences of length 146, scored 16.780\n",
      "> MSA T1026-D1_rosetta_fm has 19 sequences of length 146, scored 93.325\n",
      "> MSA T1027-D1_cov50_fm has 256 sequences of length 99, scored 65.153\n",
      "> MSA T1027-D1_rand18_fm has 203 sequences of length 99, scored 26.513\n",
      "......\n",
      "> MSA T1099-D1_cov50_fm has 256 sequences of length 178, scored 27.527\n",
      "> MSA T1099-D1_rosetta_fm has 8 sequences of length 178, scored 80.058\n",
      "> MSA T1100-D1_rand13_fm has 200 sequences of length 169, scored 60.800\n",
      "> MSA T1100-D1_rosetta_fm has 256 sequences of length 169, scored 67.750\n",
      "> MSA T1100-D2_base_fm has 256 sequences of length 157, scored 97.453\n",
      "> MSA T1100-D2_rand14_fm has 205 sequences of length 157, scored 22.453\n",
      "> MSA T1101-D1_rand13_fm has 203 sequences of length 83, scored 97.892\n",
      "> MSA T1101-D1_rand18_fm has 202 sequences of length 83, scored 99.398\n",
      "> MSA T1101-D2_base_fm has 256 sequences of length 214, scored 93.458\n",
      "> MSA T1101-D2_rand12_fm has 205 sequences of length 214, scored 25.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MSAScoreDataset(root = DATASET_PATH, is_train = False)\n",
    "test_dataset.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSAPredictor(nn.Module):\n",
    "    def __init__(self, msa_transformer_path = TRANSFORMER_PATH):\n",
    "        super(MSAPredictor, self).__init__()\n",
    "        \n",
    "        if msa_transformer_path:\n",
    "            self.encoder, msa_alphabet = esm.pretrained.load_model_and_alphabet_local(msa_transformer_path)\n",
    "        else :\n",
    "            self.encoder, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "        \n",
    "        self.encoder = self.encoder.eval()\n",
    "        self.batch_converter = msa_alphabet.get_batch_converter()\n",
    "\n",
    "        # Freeze parameters of MSATransformer\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Regressor module (to be tested)\n",
    "        # self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(3, 3)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(25232, 2048)\n",
    "        # self.fc2 = nn.Linear(2048, 512)\n",
    "        # self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_length = [list(x[i, 0]).index(1) if 1 in x[i, 0] else x.size(2) for i in range(x.size(0))]\n",
    "        \n",
    "        self.encoder\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(x.size(0)) :\n",
    "                # Remove sequence paddings\n",
    "                xi = x[i:i+1, :, :seq_length[i]]\n",
    "                # 1 x NUM_SEQ x 1+SEQ_LEN\n",
    "\n",
    "                # xx = xx[:, :list(x[i, :, 0]).index(1), :] if 1 in x[i, :, 0] else xx\n",
    "\n",
    "                xi = self.encoder(xi, repr_layers=[12])[\"representations\"][12][:, 0, 1:, :]\n",
    "                # 1 x SEQ_LEN x 768\n",
    "\n",
    "                xi = torch.mean(xi, dim = 1)\n",
    "                # 1 x 768\n",
    "                embeddings.append(xi)\n",
    "                \n",
    "        x = torch.vstack(embeddings)\n",
    "        # BATHCH_SIZE x 768\n",
    "\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MSAPredictor(\n",
       "    (encoder): MSATransformer(\n",
       "      (embed_tokens): Embedding(33, 768, padding_idx=1)\n",
       "      (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): AxialTransformerLayer(\n",
       "          (row_self_attention): NormalizedResidualBlock(\n",
       "            (layer): RowSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (column_self_attention): NormalizedResidualBlock(\n",
       "            (layer): ColumnSelfAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (feed_forward_layer): NormalizedResidualBlock(\n",
       "            (layer): FeedForwardNetwork(\n",
       "              (activation_fn): GELU(approximate=none)\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (contact_head): ContactPredictionHead(\n",
       "        (regression): Linear(in_features=144, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "      (emb_layer_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (emb_layer_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MSAPredictor().cuda()\n",
    "batch_converter = model.batch_converter\n",
    "\n",
    "NUM_GPU = torch.cuda.device_count()\n",
    "USE_PARALLEL = NUM_GPU > 1\n",
    "if USE_PARALLEL :\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_collate_fn = lambda batch: {'msa': batch_converter([msa['msa'] for msa in batch])[2], 'score': torch.Tensor([[msa['score'] / 100.0] for msa in batch])}\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, collate_fn = batch_collate_fn, num_workers = NUM_GPU * 4, pin_memory = True, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 2, collate_fn = batch_collate_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msa': tensor([[[ 0,  7,  8,  ...,  1,  1,  1],\n",
      "         [ 0, 20,  8,  ...,  1,  1,  1],\n",
      "         [ 0,  7,  8,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 20, 18,  ...,  1,  1,  1],\n",
      "         [ 0, 20, 12,  ...,  1,  1,  1],\n",
      "         [ 0, 20, 20,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 10, 16,  ...,  1,  1,  1],\n",
      "         [ 0, 10, 16,  ...,  1,  1,  1],\n",
      "         [ 0, 10, 16,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0, 20, 12,  ...,  1,  1,  1],\n",
      "         [ 0,  4, 16,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 17, 15,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
      "\n",
      "        [[ 0, 13, 18,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 18,  7,  ...,  1,  1,  1],\n",
      "         ...,\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1],\n",
      "         [ 0, 30, 30,  ...,  1,  1,  1]]]), 'score': tensor([[0.2193],\n",
      "        [0.6979],\n",
      "        [0.2355],\n",
      "        [0.3956],\n",
      "        [0.2807],\n",
      "        [0.2172],\n",
      "        [0.7204],\n",
      "        [0.7725],\n",
      "        [0.6446],\n",
      "        [0.2015],\n",
      "        [0.1405],\n",
      "        [0.8180],\n",
      "        [0.1700],\n",
      "        [0.8183],\n",
      "        [0.8073],\n",
      "        [0.7376],\n",
      "        [0.8015],\n",
      "        [0.4400],\n",
      "        [0.4445],\n",
      "        [0.6879],\n",
      "        [0.5167],\n",
      "        [0.2231],\n",
      "        [0.7869],\n",
      "        [0.7125],\n",
      "        [0.7777],\n",
      "        [0.3155],\n",
      "        [0.3546],\n",
      "        [0.4927],\n",
      "        [0.3468],\n",
      "        [0.4460],\n",
      "        [0.7129],\n",
      "        [0.5029]])}\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "print(next(dataiter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean_120 model loaded has been trained for 3 epoche(s), with 0.019078227395850016 training loss, 0.01769655714007585 validation MSE and 0.9473684210526315 test accuracy. \n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "model_name = \"mean_256\"\n",
    "checkpoint_epoch = 11\n",
    "if checkpoint_epoch > 0 :\n",
    "    checkpoint = torch.load(MODEL_PATH / f\"model_{model_name}_epoch{checkpoint_epoch}.pth\")\n",
    "    (model.module if USE_PARALLEL else model).load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"The {model_name} model loaded has been trained for {epoch} epoche(s), with {checkpoint['train_mse']} training loss, {checkpoint['valid_mse']} validation MSE and {checkpoint['test_acc']} test accuracy. \")\n",
    "else :\n",
    "    print(f\"Start training {model_name} model from the 1st epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_metric(model):\n",
    "    model.eval()\n",
    "    mse, correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total = len(test_loader), ncols=80) as bar:\n",
    "            for sample in test_loader :\n",
    "                x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "                pred = model(x)\n",
    "\n",
    "                mse += torch.sum((y - pred) ** 2).item() \n",
    "                correct += torch.sum(torch.argmax(y) == torch.argmax(pred)).item()\n",
    "                tot += 2\n",
    "\n",
    "                bar.set_postfix({\n",
    "                    \"acc\" : f\"{correct / (tot // 2):.4f}\",\n",
    "                    \"mse\": f\"{mse / tot:.4f}\"\n",
    "                })\n",
    "                bar.update(1)\n",
    "    \n",
    "    return {'accuracy' : correct / (tot // 2), 'mse' : mse / tot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch#1/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [50:34<00:00, 36.12s/it, batch loss=0.03845, loss=0.03263]\n",
      "100%|███████████████████| 95/95 [06:17<00:00,  3.98s/it, acc=0.9579, mse=0.0293]\n",
      "[epoch#2/50][2656/2660]: 100%|██████████████████████████████████| 84/84 [49:54<00:00, 35.65s/it, batch loss=0.01705, loss=0.02258]\n",
      "100%|███████████████████| 95/95 [06:17<00:00,  3.97s/it, acc=0.9474, mse=0.0203]\n",
      "[epoch#3/50][1184/2660]:  44%|██████████████▉                   | 37/84 [22:26<28:21, 36.21s/it, batch loss=0.01386, loss=0.01908]"
     ]
    }
   ],
   "source": [
    "for epoch in range(checkpoint_epoch + 1, EPOCHES + 1):\n",
    "    losses, tot = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    with tqdm(total= len(train_loader), ncols=130) as bar:\n",
    "        for batch, sample in enumerate(train_loader) :\n",
    "            bar.set_description(f\"[epoch#{epoch}/{EPOCHES}][{batch * BATCH_SIZE}/{len(train_dataset)}]\")\n",
    "\n",
    "            x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses += loss.item() * x.size(0)\n",
    "            tot += x.size(0)\n",
    "            bar.set_postfix({\n",
    "                \"batch loss\" : f\"{loss.item():.5f}\",\n",
    "                \"loss\": f\"{losses / tot:.5f}\"\n",
    "            })\n",
    "            bar.update(1)\n",
    "\n",
    "    test_metric =  calc_test_metric(model)\n",
    "    train_mse, test_acc, valid_mse =  losses / tot, test_metric['accuracy'], test_metric['mse']\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': (model.module if USE_PARALLEL else model).state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_mse': train_mse,\n",
    "        'valid_mse': valid_mse,\n",
    "        'test_acc': test_acc\n",
    "    }, MODEL_PATH / f\"model_{model_name}_epoch{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1026-D1_rand13_fm of 202 sequences is scored 16.78.\n",
      "T1026-D1_rosetta_fm of 19 sequences is scored 93.325.\n",
      "------------\n",
      "T1030-D2_base_fm of 27 sequences is scored 91.177.\n",
      "T1030-D2_original_fm of 256 sequences is scored 60.085.\n",
      "------------\n",
      "T1033-D1_base_fm of 11 sequences is scored 85.25.\n",
      "T1033-D1_rand13_fm of 209 sequences is scored 36.0.\n",
      "------------\n",
      "T1036s1-D1_our_fm of 256 sequences is scored 36.192.\n",
      "T1036s1-D1_rand19_fm of 200 sequences is scored 65.053.\n",
      "------------\n",
      "T1038-D1_base_fm of 110 sequences is scored 89.475.\n",
      "T1038-D1_rand6_fm of 207 sequences is scored 16.23.\n",
      "------------\n",
      "T1038-D2_base_fm of 70 sequences is scored 91.778.\n",
      "T1038-D2_rand13_fm of 200 sequences is scored 70.725.\n",
      "------------\n",
      "T1046s1-D1_rand13_fm of 204 sequences is scored 63.542.\n",
      "T1046s1-D1_rosetta_fm of 44 sequences is scored 97.915.\n",
      "------------\n",
      "T1046s2-D1_rand1_fm of 201 sequences is scored 21.633.\n",
      "T1046s2-D1_rosetta_fm of 52 sequences is scored 98.935.\n",
      "------------\n",
      "T1047s2-D2_rand16_fm of 204 sequences is scored 75.3.\n",
      "T1047s2-D2_rand5_fm of 202 sequences is scored 93.978.\n",
      "------------\n",
      "T1053-D1_base_fm of 256 sequences is scored 95.418.\n",
      "T1053-D1_our_fm of 256 sequences is scored 72.635.\n",
      "------------\n",
      "T1053-D2_rand13_fm of 205 sequences is scored 74.707.\n",
      "T1053-D2_rosetta_fm of 62 sequences is scored 84.065.\n",
      "------------\n",
      "T1061-D1_cov50_fm of 256 sequences is scored 77.37.\n",
      "T1061-D1_original_fm of 256 sequences is scored 55.172.\n",
      "------------\n",
      "T1062-D1_rand13_fm of 201 sequences is scored 81.43.\n",
      "T1062-D1_rosetta_fm of 18 sequences is scored 88.57.\n",
      "------------\n",
      "T1064-D1_base_fm of 48 sequences is scored 66.302.\n",
      "T1064-D1_rand5_fm of 200 sequences is scored 20.38.\n",
      "------------\n",
      "T1068-D1_our_fm of 256 sequences is scored 86.733.\n",
      "T1068-D1_rosetta_fm of 32 sequences is scored 97.207.\n",
      "------------\n",
      "T1070-D3_rand13_fm of 209 sequences is scored 73.685.\n",
      "T1070-D3_rand16_fm of 208 sequences is scored 83.225.\n",
      "------------\n",
      "T1073-D1_rand12_fm of 200 sequences is scored 92.797.\n",
      "T1073-D1_rand13_fm of 203 sequences is scored 89.41.\n",
      "------------\n",
      "T1082-D1_aug_fm of 126 sequences is scored 40.0.\n",
      "T1082-D1_rosetta_fm of 11 sequences is scored 96.668.\n",
      "------------\n",
      "T1093-D3_base_fm of 44 sequences is scored 92.69.\n",
      "T1093-D3_rand13_fm of 201 sequences is scored 22.17.\n",
      "------------\n",
      "T1099-D1_cov50_fm of 256 sequences is scored 27.527.\n",
      "T1099-D1_rosetta_fm of 8 sequences is scored 80.058.\n",
      "------------\n",
      "T1101-D1_rand13_fm of 203 sequences is scored 97.892.\n",
      "T1101-D1_rand18_fm of 202 sequences is scored 99.398.\n",
      "------------\n",
      "0.7789473684210526\n"
     ]
    }
   ],
   "source": [
    "# Baseline: output the one with more sequences\n",
    "def calc_baseline_test_acc(test_dataset):\n",
    "    correct = 0\n",
    "    pairs = len(test_dataset) // 2\n",
    "    for i in range(pairs) :\n",
    "        msa1, msa2 = test_dataset[2*i], test_dataset[2*i+1]\n",
    "        pred = len(msa1['msa']) > len(msa2['msa'])\n",
    "        gt = msa1['score'] > msa2['score']\n",
    "        if pred == gt : \n",
    "            correct += 1\n",
    "        else :\n",
    "            print(f\"{msa1['name']} of {len(msa1['msa'])} sequences is scored {msa1['score']}.\")\n",
    "            print(f\"{msa2['name']} of {len(msa2['msa'])} sequences is scored {msa2['score']}.\")\n",
    "            print(\"------------\")\n",
    "    return correct / pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256x 155 y_gt:85.7150  y_pred:86.6433                                          \n",
      " 203x 155 y_gt:81.4950  y_pred:86.6760                                          \n",
      "-------                                                                         \n",
      "  27x 120 y_gt:91.1770  y_pred:64.8868                                          \n",
      " 256x 120 y_gt:60.0850  y_pred:68.9962                                          \n",
      "-------                                                                         \n",
      "  11x 101 y_gt:85.2500  y_pred:56.1897                                          \n",
      " 256x 101 y_gt:36.0000  y_pred:63.0460                                          \n",
      "-------                                                                         \n",
      " 27%|█████▏             | 26/95 [01:46<03:41,  3.21s/it, acc=0.8846, mse=0.0170]"
     ]
    }
   ],
   "source": [
    "def print_wrong_predictions(model):\n",
    "    model.eval()\n",
    "    mse, correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total = len(test_loader), ncols=80, file=sys.stdout) as bar:\n",
    "            for sample in test_loader :\n",
    "                x, y = sample[\"msa\"].cuda(non_blocking = True), sample[\"score\"].cuda()\n",
    "                pred = model(x)\n",
    "\n",
    "                mse += torch.sum((y - pred) ** 2).item() \n",
    "                if torch.argmax(y) == torch.argmax(pred) :\n",
    "                    correct += 1\n",
    "                    for x, y_gt, y_pred in zip(x, y, pred) :\n",
    "                        tqdm.write(f\"{list(x[:, 0]).index(1) if 1 in x[:, 0] else 256:>4d}x{x.size(1):>4d} y_gt:{y_gt.item():.4f}  y_pred:{y_pred.item() * 120:.4f}\")\n",
    "                    tqdm.write(\"======\")\n",
    "                else :\n",
    "                    for x, y_gt, y_pred in zip(x, y, pred) :\n",
    "                        tqdm.write(f\"{list(x[:, 0]).index(1) if 1 in x[:, 0] else 256:>4d}x{x.size(1):>4d} y_gt:{y_gt.item():.4f}  y_pred:{y_pred.item() * 120:.4f}\")\n",
    "                    tqdm.write(\"-------\")\n",
    "                tot += 2\n",
    "\n",
    "                bar.set_postfix({\n",
    "                    \"acc\" : f\"{correct / (tot // 2):.4f}\",\n",
    "                    \"mse\": f\"{mse / tot:.4f}\"\n",
    "                })\n",
    "                bar.update(1)\n",
    "    \n",
    "    return {'accuracy' : correct / (tot // 2), 'mse' : mse / tot}\n",
    "\n",
    "#print_wrong_predictions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:54<00:00,  1.81s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_history(model_name = \"mean_256\", trained_epoches = 10):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    for epoch in tqdm(range(1, trained_epoches+1)):\n",
    "        checkpoint = torch.load(MODEL_PATH / f\"model_{model_name}_epoch{epoch}.pth\")\n",
    "        train_losses.append(checkpoint[\"train_mse\"])\n",
    "        test_losses.append(checkpoint[\"valid_mse\"])\n",
    "        test_accs.append(checkpoint[\"test_acc\"] / 20)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, \"r-o\", label = \"train loss\")\n",
    "    plt.plot(test_losses, \"b-o\", label = \"valid MSE\")\n",
    "    plt.plot(test_accs, \"g-o\", label = \"test accuracy\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#plot_training_history(\"mean_256\", 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e0b1f18ae03a7035bc033a9f9c7ae76394b92e608ff6812adcccfdadc13d2c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
